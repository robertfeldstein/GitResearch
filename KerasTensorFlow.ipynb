{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras.layers import Flatten\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>Coordinates</th>\n",
       "      <th>CMI_C01</th>\n",
       "      <th>CMI_C02</th>\n",
       "      <th>CMI_C03</th>\n",
       "      <th>CMI_C04</th>\n",
       "      <th>CMI_C05</th>\n",
       "      <th>CMI_C06</th>\n",
       "      <th>CMI_C07</th>\n",
       "      <th>...</th>\n",
       "      <th>CMI_C11</th>\n",
       "      <th>CMI_C12</th>\n",
       "      <th>CMI_C13</th>\n",
       "      <th>CMI_C14</th>\n",
       "      <th>CMI_C15</th>\n",
       "      <th>CMI_C16</th>\n",
       "      <th>ACM</th>\n",
       "      <th>BCM</th>\n",
       "      <th>Cloud_Probabilities</th>\n",
       "      <th>Lightning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(44.28533797848896, -96.33244120270002)</td>\n",
       "      <td>0.169623</td>\n",
       "      <td>0.099563</td>\n",
       "      <td>0.375139</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.195774</td>\n",
       "      <td>0.102440</td>\n",
       "      <td>298.40840</td>\n",
       "      <td>...</td>\n",
       "      <td>288.29090</td>\n",
       "      <td>257.63177</td>\n",
       "      <td>292.63490</td>\n",
       "      <td>292.07776</td>\n",
       "      <td>288.22775</td>\n",
       "      <td>269.13647</td>\n",
       "      <td>0.078123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029264</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(44.16192918412268, -96.2790076055067)</td>\n",
       "      <td>0.180972</td>\n",
       "      <td>0.110357</td>\n",
       "      <td>0.416170</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.207321</td>\n",
       "      <td>0.111607</td>\n",
       "      <td>298.71533</td>\n",
       "      <td>...</td>\n",
       "      <td>287.71057</td>\n",
       "      <td>257.37183</td>\n",
       "      <td>291.90130</td>\n",
       "      <td>291.38574</td>\n",
       "      <td>287.57257</td>\n",
       "      <td>268.87482</td>\n",
       "      <td>0.109530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046814</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>(44.03893135837276, -96.22611143587908)</td>\n",
       "      <td>0.240833</td>\n",
       "      <td>0.172460</td>\n",
       "      <td>0.445059</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.238750</td>\n",
       "      <td>0.145952</td>\n",
       "      <td>298.25534</td>\n",
       "      <td>...</td>\n",
       "      <td>285.00128</td>\n",
       "      <td>255.86801</td>\n",
       "      <td>288.94000</td>\n",
       "      <td>288.50170</td>\n",
       "      <td>285.11570</td>\n",
       "      <td>267.86960</td>\n",
       "      <td>0.813740</td>\n",
       "      <td>0.094060</td>\n",
       "      <td>0.265362</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>(43.916339862914086, -96.17374408076766)</td>\n",
       "      <td>0.245674</td>\n",
       "      <td>0.177897</td>\n",
       "      <td>0.430456</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.237897</td>\n",
       "      <td>0.147659</td>\n",
       "      <td>298.69244</td>\n",
       "      <td>...</td>\n",
       "      <td>285.33386</td>\n",
       "      <td>256.09546</td>\n",
       "      <td>289.18967</td>\n",
       "      <td>288.67377</td>\n",
       "      <td>285.32043</td>\n",
       "      <td>267.93500</td>\n",
       "      <td>0.687891</td>\n",
       "      <td>0.093906</td>\n",
       "      <td>0.228131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>(43.794150142168, -96.1218971247137)</td>\n",
       "      <td>0.252857</td>\n",
       "      <td>0.182976</td>\n",
       "      <td>0.475952</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.140972</td>\n",
       "      <td>297.63165</td>\n",
       "      <td>...</td>\n",
       "      <td>284.73395</td>\n",
       "      <td>255.71437</td>\n",
       "      <td>288.62890</td>\n",
       "      <td>288.18744</td>\n",
       "      <td>284.90723</td>\n",
       "      <td>267.90747</td>\n",
       "      <td>0.797112</td>\n",
       "      <td>0.125466</td>\n",
       "      <td>0.303975</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                               Coordinates   CMI_C01  \\\n",
       "0           0      0   (44.28533797848896, -96.33244120270002)  0.169623   \n",
       "1           1      1    (44.16192918412268, -96.2790076055067)  0.180972   \n",
       "2           2      2   (44.03893135837276, -96.22611143587908)  0.240833   \n",
       "3           3      3  (43.916339862914086, -96.17374408076766)  0.245674   \n",
       "4           4      4      (43.794150142168, -96.1218971247137)  0.252857   \n",
       "\n",
       "    CMI_C02   CMI_C03   CMI_C04   CMI_C05   CMI_C06    CMI_C07  ...  \\\n",
       "0  0.099563  0.375139  0.001111  0.195774  0.102440  298.40840  ...   \n",
       "1  0.110357  0.416170  0.001071  0.207321  0.111607  298.71533  ...   \n",
       "2  0.172460  0.445059  0.001111  0.238750  0.145952  298.25534  ...   \n",
       "3  0.177897  0.430456  0.001587  0.237897  0.147659  298.69244  ...   \n",
       "4  0.182976  0.475952  0.001270  0.233353  0.140972  297.63165  ...   \n",
       "\n",
       "     CMI_C11    CMI_C12    CMI_C13    CMI_C14    CMI_C15    CMI_C16       ACM  \\\n",
       "0  288.29090  257.63177  292.63490  292.07776  288.22775  269.13647  0.078123   \n",
       "1  287.71057  257.37183  291.90130  291.38574  287.57257  268.87482  0.109530   \n",
       "2  285.00128  255.86801  288.94000  288.50170  285.11570  267.86960  0.813740   \n",
       "3  285.33386  256.09546  289.18967  288.67377  285.32043  267.93500  0.687891   \n",
       "4  284.73395  255.71437  288.62890  288.18744  284.90723  267.90747  0.797112   \n",
       "\n",
       "        BCM  Cloud_Probabilities  Lightning  \n",
       "0  0.000000             0.029264          0  \n",
       "1  0.000000             0.046814          0  \n",
       "2  0.094060             0.265362          0  \n",
       "3  0.093906             0.228131          0  \n",
       "4  0.125466             0.303975          0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/robbiefeldstein/Documents/Programming/Research/Datasets/2023-10-11.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "    Total: 937500\n",
      "    Positive: 540 (0.06% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Look at class imbalance\n",
    "\n",
    "neg, pos = np.bincount(df['Lightning'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"CMI_C01\", \"CMI_C02\", \"CMI_C03\",\"CMI_C04\", \"CMI_C05\",\"CMI_C06\", \"CMI_C07\",\"CMI_C15\",\"Cloud_Probabilities\",\"Lightning\"]\n",
    "#let's just do less features\n",
    "#Predictors\n",
    "\n",
    "copy_df = df.copy()\n",
    "copy_df = copy_df[features]\n",
    "\n",
    "X = copy_df[features]\n",
    "\n",
    "# Use a utility from sklearn to split and shuffle your dataset.\n",
    "train_df, test_df = train_test_split(copy_df, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('Lightning'))\n",
    "bool_train_labels = train_labels != 0\n",
    "val_labels = np.array(val_df.pop('Lightning'))\n",
    "test_labels = np.array(test_df.pop('Lightning'))\n",
    "\n",
    "train_features = np.array(train_df)\n",
    "val_features = np.array(val_df)\n",
    "test_features = np.array(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average class probability in training set:   0.0006\n",
      "Average class probability in validation set: 0.0006\n",
      "Average class probability in test set:       0.0006\n"
     ]
    }
   ],
   "source": [
    "#Averages are roughly similar\n",
    "\n",
    "print(f'Average class probability in training set:   {train_labels.mean():.4f}')\n",
    "print(f'Average class probability in validation set: {val_labels.mean():.4f}')\n",
    "print(f'Average class probability in test set:       {test_labels.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (600000,)\n",
      "Validation labels shape: (150000,)\n",
      "Test labels shape: (187500,)\n",
      "Training features shape: (600000, 9)\n",
      "Validation features shape: (150000, 9)\n",
      "Test features shape: (187500, 9)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "train_features = np.clip(train_features, -5, 5)\n",
    "val_features = np.clip(val_features, -5, 5)\n",
    "test_features = np.clip(test_features, -5, 5)\n",
    "\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 15:04:39.964009: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2023-10-22 15:04:39.964027: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2023-10-22 15:04:39.964032: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2023-10-22 15:04:39.964058: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-22 15:04:39.964072: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#Recommended parameters for imbalanced model\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.BinaryCrossentropy(name='cross entropy'),  # same as model's loss\n",
    "      keras.metrics.MeanSquaredError(name='Brier score'),\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "def make_model(metrics=METRICS, output_bias=None):\n",
    "  if output_bias is not None:\n",
    "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "\n",
    "  model = keras.Sequential([\n",
    "    keras.layers.Dense(len(features), activation='relu', input_shape=(train_features.shape[-1],)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(4, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(2, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    #Output layer\n",
    "    keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)\n",
    "])\n",
    "\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.legacy.Adam(learning_rate=1e-3),\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=[tf.keras.metrics.FalsePositives(), tf.keras.metrics.Precision()])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 128\n",
    "BATCH_SIZE = 16384\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.4594029]\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                100       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 10)                40        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                176       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 10        \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 501 (1.96 KB)\n",
      "Trainable params: 481 (1.88 KB)\n",
      "Non-trainable params: 20 (80.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initial_bias = np.log([pos/len(df)])\n",
    "print(initial_bias)\n",
    "model = make_model(output_bias=initial_bias)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\n",
    "model.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 15:04:41.292216: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 15:04:44.289531: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 512ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00113621],\n",
       "       [0.00063685],\n",
       "       [0.00053743],\n",
       "       [0.00050525],\n",
       "       [0.00060403],\n",
       "       [0.00207562],\n",
       "       [0.00011642],\n",
       "       [0.0001387 ],\n",
       "       [0.00103384],\n",
       "       [0.00051855]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "print(\"Loss: {:0.4f}\".format(results[0]))\n",
    "\n",
    "model = make_model(output_bias=initial_bias)\n",
    "model.predict(train_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = make_model()\n",
    "# model.load_weights(initial_weights)\n",
    "# model.layers[-1].bias.assign([0.0])\n",
    "# zero_bias_history = model.fit(\n",
    "#     train_features,\n",
    "#     train_labels,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     epochs=16,\n",
    "#     validation_data=(val_features, val_labels), \n",
    "#     verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = make_model()\n",
    "# model.load_weights(initial_weights)\n",
    "# careful_bias_history = model.fit(\n",
    "#     train_features,\n",
    "#     train_labels,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     epochs=16,\n",
    "#     validation_data=(val_features, val_labels), \n",
    "#     verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, label, n):\n",
    "  # Use a log scale on y-axis to show the wide range of values.\n",
    "  plt.semilogy(history.epoch, history.history['loss'],\n",
    "               color=colors[n], label='Train ' + label)\n",
    "  plt.semilogy(history.epoch, history.history['val_loss'],\n",
    "               color=colors[n], label='Val ' + label,\n",
    "               linestyle=\"--\")\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_loss(zero_bias_history, \"Zero Bias\", 0)\n",
    "# plot_loss(careful_bias_history, \"Careful Bias\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "  metrics = ['loss', 'precision',]\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0, plt.ylim()[1]])\n",
    "    elif metric == 'auc':\n",
    "      plt.ylim([0.8,1])\n",
    "    else:\n",
    "      plt.ylim([0,1])\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 7ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, threshold=0.5):\n",
    "  cm = confusion_matrix(labels, predictions > threshold)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(threshold))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class 0: 0.50\n",
      "Weight for class 1: 868.06\n"
     ]
    }
   ],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 15:04:46.370465: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - ETA: 0s - loss: 3.5895 - false_positives_2: 21335.0000 - precision_2: 0.0014"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 15:04:55.130504: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 12s 170ms/step - loss: 3.5895 - false_positives_2: 21335.0000 - precision_2: 0.0014 - val_loss: 0.0047 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 2/250\n",
      "37/37 [==============================] - 1s 19ms/step - loss: 3.6315 - false_positives_2: 20100.0000 - precision_2: 0.0018 - val_loss: 0.0048 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 3/250\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 3.5324 - false_positives_2: 20796.0000 - precision_2: 0.0012 - val_loss: 0.0048 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 4/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 3.4945 - false_positives_2: 21192.0000 - precision_2: 0.0013 - val_loss: 0.0049 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 5/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 3.5370 - false_positives_2: 21064.0000 - precision_2: 0.0016 - val_loss: 0.0050 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 6/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 3.6404 - false_positives_2: 20313.0000 - precision_2: 0.0016 - val_loss: 0.0051 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 7/250\n",
      "37/37 [==============================] - 1s 19ms/step - loss: 3.4466 - false_positives_2: 20358.0000 - precision_2: 0.0015 - val_loss: 0.0051 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 8/250\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 3.3858 - false_positives_2: 20131.0000 - precision_2: 0.0015 - val_loss: 0.0052 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 9/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 3.4347 - false_positives_2: 19287.0000 - precision_2: 0.0019 - val_loss: 0.0053 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 10/250\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 3.5061 - false_positives_2: 19494.0000 - precision_2: 0.0012 - val_loss: 0.0054 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 11/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 3.1903 - false_positives_2: 19607.0000 - precision_2: 0.0016 - val_loss: 0.0055 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 12/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 3.2821 - false_positives_2: 20475.0000 - precision_2: 0.0016 - val_loss: 0.0056 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 13/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 3.3084 - false_positives_2: 20389.0000 - precision_2: 0.0012 - val_loss: 0.0057 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 14/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 3.2554 - false_positives_2: 20607.0000 - precision_2: 0.0011 - val_loss: 0.0057 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 15/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 3.0796 - false_positives_2: 19917.0000 - precision_2: 0.0012 - val_loss: 0.0058 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 16/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 3.1520 - false_positives_2: 20014.0000 - precision_2: 0.0014 - val_loss: 0.0059 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 17/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 2.9537 - false_positives_2: 20437.0000 - precision_2: 0.0013 - val_loss: 0.0060 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 18/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 3.1087 - false_positives_2: 21164.0000 - precision_2: 0.0012 - val_loss: 0.0062 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 19/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 2.8209 - false_positives_2: 21204.0000 - precision_2: 0.0019 - val_loss: 0.0063 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 20/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 2.8381 - false_positives_2: 22291.0000 - precision_2: 0.0012 - val_loss: 0.0065 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 21/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 2.7008 - false_positives_2: 24426.0000 - precision_2: 0.0014 - val_loss: 0.0070 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 22/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 2.5602 - false_positives_2: 27950.0000 - precision_2: 0.0021 - val_loss: 0.0078 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 23/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 2.4767 - false_positives_2: 29943.0000 - precision_2: 0.0021 - val_loss: 0.0099 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 24/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 2.4877 - false_positives_2: 33448.0000 - precision_2: 0.0017 - val_loss: 0.0153 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 25/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 2.4541 - false_positives_2: 36350.0000 - precision_2: 0.0020 - val_loss: 0.0214 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 26/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 2.2662 - false_positives_2: 37604.0000 - precision_2: 0.0025 - val_loss: 0.0272 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 27/250\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 2.1970 - false_positives_2: 39333.0000 - precision_2: 0.0021 - val_loss: 0.0363 - val_false_positives_2: 16.0000 - val_precision_2: 0.0000e+00\n",
      "Epoch 28/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 2.0363 - false_positives_2: 41344.0000 - precision_2: 0.0026 - val_loss: 0.0475 - val_false_positives_2: 772.0000 - val_precision_2: 0.0013\n",
      "Epoch 29/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 2.0688 - false_positives_2: 43920.0000 - precision_2: 0.0021 - val_loss: 0.0642 - val_false_positives_2: 4136.0000 - val_precision_2: 9.6618e-04\n",
      "Epoch 30/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.9530 - false_positives_2: 46172.0000 - precision_2: 0.0023 - val_loss: 0.0882 - val_false_positives_2: 8247.0000 - val_precision_2: 0.0034\n",
      "Epoch 31/250\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 2.0321 - false_positives_2: 47672.0000 - precision_2: 0.0024 - val_loss: 0.1156 - val_false_positives_2: 10822.0000 - val_precision_2: 0.0044\n",
      "Epoch 32/250\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 2.0151 - false_positives_2: 48846.0000 - precision_2: 0.0022 - val_loss: 0.1229 - val_false_positives_2: 11331.0000 - val_precision_2: 0.0043\n",
      "Epoch 33/250\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 1.9213 - false_positives_2: 49092.0000 - precision_2: 0.0025 - val_loss: 0.1213 - val_false_positives_2: 11229.0000 - val_precision_2: 0.0043\n",
      "Epoch 34/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.9133 - false_positives_2: 50504.0000 - precision_2: 0.0024 - val_loss: 0.1615 - val_false_positives_2: 13337.0000 - val_precision_2: 0.0047\n",
      "Epoch 35/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.8486 - false_positives_2: 50544.0000 - precision_2: 0.0026 - val_loss: 0.1757 - val_false_positives_2: 13836.0000 - val_precision_2: 0.0047\n",
      "Epoch 36/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.9103 - false_positives_2: 50549.0000 - precision_2: 0.0025 - val_loss: 0.1713 - val_false_positives_2: 13658.0000 - val_precision_2: 0.0046\n",
      "Epoch 37/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 2.0387 - false_positives_2: 50089.0000 - precision_2: 0.0023 - val_loss: 0.1796 - val_false_positives_2: 13950.0000 - val_precision_2: 0.0046\n",
      "Epoch 38/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.9492 - false_positives_2: 49554.0000 - precision_2: 0.0024 - val_loss: 0.1630 - val_false_positives_2: 13330.0000 - val_precision_2: 0.0047\n",
      "Epoch 39/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.9743 - false_positives_2: 49463.0000 - precision_2: 0.0023 - val_loss: 0.1976 - val_false_positives_2: 14501.0000 - val_precision_2: 0.0047\n",
      "Epoch 40/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.7710 - false_positives_2: 51011.0000 - precision_2: 0.0028 - val_loss: 0.2175 - val_false_positives_2: 15109.0000 - val_precision_2: 0.0047\n",
      "Epoch 41/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.7580 - false_positives_2: 51567.0000 - precision_2: 0.0026 - val_loss: 0.2281 - val_false_positives_2: 15429.0000 - val_precision_2: 0.0047\n",
      "Epoch 42/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.7001 - false_positives_2: 51485.0000 - precision_2: 0.0028 - val_loss: 0.2240 - val_false_positives_2: 15374.0000 - val_precision_2: 0.0047\n",
      "Epoch 43/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.8741 - false_positives_2: 51949.0000 - precision_2: 0.0023 - val_loss: 0.2345 - val_false_positives_2: 15606.0000 - val_precision_2: 0.0047\n",
      "Epoch 44/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.8148 - false_positives_2: 51355.0000 - precision_2: 0.0025 - val_loss: 0.2304 - val_false_positives_2: 15506.0000 - val_precision_2: 0.0047\n",
      "Epoch 45/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.7281 - false_positives_2: 50851.0000 - precision_2: 0.0027 - val_loss: 0.2514 - val_false_positives_2: 15961.0000 - val_precision_2: 0.0047\n",
      "Epoch 46/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.8660 - false_positives_2: 52600.0000 - precision_2: 0.0024 - val_loss: 0.2817 - val_false_positives_2: 16546.0000 - val_precision_2: 0.0047\n",
      "Epoch 47/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.7563 - false_positives_2: 53039.0000 - precision_2: 0.0025 - val_loss: 0.2863 - val_false_positives_2: 16663.0000 - val_precision_2: 0.0047\n",
      "Epoch 48/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.7543 - false_positives_2: 51644.0000 - precision_2: 0.0026 - val_loss: 0.2417 - val_false_positives_2: 15747.0000 - val_precision_2: 0.0047\n",
      "Epoch 49/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.7253 - false_positives_2: 49931.0000 - precision_2: 0.0029 - val_loss: 0.2299 - val_false_positives_2: 15454.0000 - val_precision_2: 0.0047\n",
      "Epoch 50/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.7139 - false_positives_2: 49276.0000 - precision_2: 0.0027 - val_loss: 0.2383 - val_false_positives_2: 15618.0000 - val_precision_2: 0.0047\n",
      "Epoch 51/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.6944 - false_positives_2: 49701.0000 - precision_2: 0.0027 - val_loss: 0.2343 - val_false_positives_2: 15578.0000 - val_precision_2: 0.0047\n",
      "Epoch 52/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.5661 - false_positives_2: 49339.0000 - precision_2: 0.0030 - val_loss: 0.2444 - val_false_positives_2: 15739.0000 - val_precision_2: 0.0047\n",
      "Epoch 53/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.6244 - false_positives_2: 48496.0000 - precision_2: 0.0030 - val_loss: 0.2359 - val_false_positives_2: 15559.0000 - val_precision_2: 0.0047\n",
      "Epoch 54/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.6588 - false_positives_2: 48417.0000 - precision_2: 0.0026 - val_loss: 0.2409 - val_false_positives_2: 15687.0000 - val_precision_2: 0.0046\n",
      "Epoch 55/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.5682 - false_positives_2: 49035.0000 - precision_2: 0.0028 - val_loss: 0.2557 - val_false_positives_2: 16096.0000 - val_precision_2: 0.0046\n",
      "Epoch 56/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.5573 - false_positives_2: 49085.0000 - precision_2: 0.0030 - val_loss: 0.2280 - val_false_positives_2: 15496.0000 - val_precision_2: 0.0047\n",
      "Epoch 57/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.5945 - false_positives_2: 48122.0000 - precision_2: 0.0029 - val_loss: 0.2084 - val_false_positives_2: 14979.0000 - val_precision_2: 0.0047\n",
      "Epoch 58/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.6392 - false_positives_2: 48265.0000 - precision_2: 0.0028 - val_loss: 0.2108 - val_false_positives_2: 15051.0000 - val_precision_2: 0.0047\n",
      "Epoch 59/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.6441 - false_positives_2: 46650.0000 - precision_2: 0.0029 - val_loss: 0.1888 - val_false_positives_2: 14416.0000 - val_precision_2: 0.0047\n",
      "Epoch 60/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.6074 - false_positives_2: 45950.0000 - precision_2: 0.0027 - val_loss: 0.1976 - val_false_positives_2: 14709.0000 - val_precision_2: 0.0047\n",
      "Epoch 61/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.4986 - false_positives_2: 47171.0000 - precision_2: 0.0030 - val_loss: 0.2080 - val_false_positives_2: 15056.0000 - val_precision_2: 0.0047\n",
      "Epoch 62/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.5021 - false_positives_2: 47580.0000 - precision_2: 0.0030 - val_loss: 0.2095 - val_false_positives_2: 15147.0000 - val_precision_2: 0.0047\n",
      "Epoch 63/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.5368 - false_positives_2: 47499.0000 - precision_2: 0.0028 - val_loss: 0.1964 - val_false_positives_2: 14780.0000 - val_precision_2: 0.0046\n",
      "Epoch 64/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.5649 - false_positives_2: 46536.0000 - precision_2: 0.0028 - val_loss: 0.1762 - val_false_positives_2: 14119.0000 - val_precision_2: 0.0047\n",
      "Epoch 65/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.4930 - false_positives_2: 46328.0000 - precision_2: 0.0031 - val_loss: 0.1767 - val_false_positives_2: 14165.0000 - val_precision_2: 0.0046\n",
      "Epoch 66/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.5310 - false_positives_2: 45308.0000 - precision_2: 0.0029 - val_loss: 0.1754 - val_false_positives_2: 14185.0000 - val_precision_2: 0.0047\n",
      "Epoch 67/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.4989 - false_positives_2: 46579.0000 - precision_2: 0.0028 - val_loss: 0.1797 - val_false_positives_2: 14341.0000 - val_precision_2: 0.0047\n",
      "Epoch 68/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.3442 - false_positives_2: 46366.0000 - precision_2: 0.0033 - val_loss: 0.1680 - val_false_positives_2: 13926.0000 - val_precision_2: 0.0047\n",
      "Epoch 69/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.5287 - false_positives_2: 46402.0000 - precision_2: 0.0029 - val_loss: 0.1641 - val_false_positives_2: 13804.0000 - val_precision_2: 0.0048\n",
      "Epoch 70/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.6083 - false_positives_2: 46033.0000 - precision_2: 0.0026 - val_loss: 0.1608 - val_false_positives_2: 13651.0000 - val_precision_2: 0.0046\n",
      "Epoch 71/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.5223 - false_positives_2: 45910.0000 - precision_2: 0.0025 - val_loss: 0.1739 - val_false_positives_2: 14208.0000 - val_precision_2: 0.0047\n",
      "Epoch 72/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.4025 - false_positives_2: 46782.0000 - precision_2: 0.0030 - val_loss: 0.1860 - val_false_positives_2: 14740.0000 - val_precision_2: 0.0046\n",
      "Epoch 73/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.4604 - false_positives_2: 47440.0000 - precision_2: 0.0029 - val_loss: 0.1919 - val_false_positives_2: 14923.0000 - val_precision_2: 0.0047\n",
      "Epoch 74/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.4936 - false_positives_2: 47589.0000 - precision_2: 0.0027 - val_loss: 0.1941 - val_false_positives_2: 15007.0000 - val_precision_2: 0.0047\n",
      "Epoch 75/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.4036 - false_positives_2: 47927.0000 - precision_2: 0.0029 - val_loss: 0.1925 - val_false_positives_2: 14994.0000 - val_precision_2: 0.0047\n",
      "Epoch 76/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.4348 - false_positives_2: 48602.0000 - precision_2: 0.0025 - val_loss: 0.1944 - val_false_positives_2: 15069.0000 - val_precision_2: 0.0048\n",
      "Epoch 77/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.3624 - false_positives_2: 48744.0000 - precision_2: 0.0029 - val_loss: 0.1822 - val_false_positives_2: 14654.0000 - val_precision_2: 0.0047\n",
      "Epoch 78/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.3994 - false_positives_2: 47616.0000 - precision_2: 0.0028 - val_loss: 0.1704 - val_false_positives_2: 14199.0000 - val_precision_2: 0.0047\n",
      "Epoch 79/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.3361 - false_positives_2: 47472.0000 - precision_2: 0.0029 - val_loss: 0.1729 - val_false_positives_2: 14302.0000 - val_precision_2: 0.0047\n",
      "Epoch 80/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.3361 - false_positives_2: 47710.0000 - precision_2: 0.0030 - val_loss: 0.1725 - val_false_positives_2: 14304.0000 - val_precision_2: 0.0047\n",
      "Epoch 81/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.3417 - false_positives_2: 47993.0000 - precision_2: 0.0028 - val_loss: 0.1752 - val_false_positives_2: 14443.0000 - val_precision_2: 0.0046\n",
      "Epoch 82/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.3254 - false_positives_2: 48813.0000 - precision_2: 0.0027 - val_loss: 0.1797 - val_false_positives_2: 14647.0000 - val_precision_2: 0.0046\n",
      "Epoch 83/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.3473 - false_positives_2: 48405.0000 - precision_2: 0.0027 - val_loss: 0.1644 - val_false_positives_2: 13956.0000 - val_precision_2: 0.0047\n",
      "Epoch 84/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.2694 - false_positives_2: 47246.0000 - precision_2: 0.0030 - val_loss: 0.1564 - val_false_positives_2: 13552.0000 - val_precision_2: 0.0046\n",
      "Epoch 85/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.2796 - false_positives_2: 47586.0000 - precision_2: 0.0029 - val_loss: 0.1746 - val_false_positives_2: 14418.0000 - val_precision_2: 0.0046\n",
      "Epoch 86/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.3560 - false_positives_2: 48682.0000 - precision_2: 0.0026 - val_loss: 0.1701 - val_false_positives_2: 14219.0000 - val_precision_2: 0.0047\n",
      "Epoch 87/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.2853 - false_positives_2: 49006.0000 - precision_2: 0.0026 - val_loss: 0.1810 - val_false_positives_2: 14736.0000 - val_precision_2: 0.0047\n",
      "Epoch 88/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.2490 - false_positives_2: 49173.0000 - precision_2: 0.0027 - val_loss: 0.1707 - val_false_positives_2: 14290.0000 - val_precision_2: 0.0047\n",
      "Epoch 89/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.2189 - false_positives_2: 50004.0000 - precision_2: 0.0031 - val_loss: 0.1708 - val_false_positives_2: 14320.0000 - val_precision_2: 0.0047\n",
      "Epoch 90/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.2446 - false_positives_2: 49421.0000 - precision_2: 0.0026 - val_loss: 0.1800 - val_false_positives_2: 14808.0000 - val_precision_2: 0.0046\n",
      "Epoch 91/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.2483 - false_positives_2: 51525.0000 - precision_2: 0.0026 - val_loss: 0.1857 - val_false_positives_2: 15066.0000 - val_precision_2: 0.0048\n",
      "Epoch 92/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.1969 - false_positives_2: 50927.0000 - precision_2: 0.0029 - val_loss: 0.1780 - val_false_positives_2: 14788.0000 - val_precision_2: 0.0046\n",
      "Epoch 93/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.1928 - false_positives_2: 52158.0000 - precision_2: 0.0027 - val_loss: 0.1886 - val_false_positives_2: 15325.0000 - val_precision_2: 0.0047\n",
      "Epoch 94/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.1631 - false_positives_2: 52030.0000 - precision_2: 0.0029 - val_loss: 0.1795 - val_false_positives_2: 14923.0000 - val_precision_2: 0.0047\n",
      "Epoch 95/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.2371 - false_positives_2: 50876.0000 - precision_2: 0.0026 - val_loss: 0.1725 - val_false_positives_2: 14562.0000 - val_precision_2: 0.0046\n",
      "Epoch 96/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.2465 - false_positives_2: 50949.0000 - precision_2: 0.0026 - val_loss: 0.1804 - val_false_positives_2: 15022.0000 - val_precision_2: 0.0047\n",
      "Epoch 97/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.0947 - false_positives_2: 51555.0000 - precision_2: 0.0030 - val_loss: 0.1879 - val_false_positives_2: 15408.0000 - val_precision_2: 0.0047\n",
      "Epoch 98/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.1621 - false_positives_2: 53729.0000 - precision_2: 0.0028 - val_loss: 0.1899 - val_false_positives_2: 15533.0000 - val_precision_2: 0.0046\n",
      "Epoch 99/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.1292 - false_positives_2: 52950.0000 - precision_2: 0.0027 - val_loss: 0.1853 - val_false_positives_2: 15319.0000 - val_precision_2: 0.0047\n",
      "Epoch 100/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.1512 - false_positives_2: 52136.0000 - precision_2: 0.0027 - val_loss: 0.1730 - val_false_positives_2: 14611.0000 - val_precision_2: 0.0047\n",
      "Epoch 101/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.1365 - false_positives_2: 51529.0000 - precision_2: 0.0028 - val_loss: 0.1737 - val_false_positives_2: 14645.0000 - val_precision_2: 0.0047\n",
      "Epoch 102/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.0700 - false_positives_2: 51835.0000 - precision_2: 0.0029 - val_loss: 0.1821 - val_false_positives_2: 15161.0000 - val_precision_2: 0.0047\n",
      "Epoch 103/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.1372 - false_positives_2: 53468.0000 - precision_2: 0.0027 - val_loss: 0.1947 - val_false_positives_2: 15836.0000 - val_precision_2: 0.0047\n",
      "Epoch 104/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.0871 - false_positives_2: 55429.0000 - precision_2: 0.0027 - val_loss: 0.2024 - val_false_positives_2: 16214.0000 - val_precision_2: 0.0047\n",
      "Epoch 105/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.1429 - false_positives_2: 52813.0000 - precision_2: 0.0028 - val_loss: 0.1822 - val_false_positives_2: 15173.0000 - val_precision_2: 0.0047\n",
      "Epoch 106/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.1194 - false_positives_2: 52758.0000 - precision_2: 0.0027 - val_loss: 0.1793 - val_false_positives_2: 14991.0000 - val_precision_2: 0.0047\n",
      "Epoch 107/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.1205 - false_positives_2: 53251.0000 - precision_2: 0.0025 - val_loss: 0.1913 - val_false_positives_2: 15698.0000 - val_precision_2: 0.0048\n",
      "Epoch 108/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.0939 - false_positives_2: 52484.0000 - precision_2: 0.0028 - val_loss: 0.1866 - val_false_positives_2: 15459.0000 - val_precision_2: 0.0047\n",
      "Epoch 109/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9883 - false_positives_2: 54803.0000 - precision_2: 0.0029 - val_loss: 0.2016 - val_false_positives_2: 16224.0000 - val_precision_2: 0.0046\n",
      "Epoch 110/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.0375 - false_positives_2: 52973.0000 - precision_2: 0.0029 - val_loss: 0.1856 - val_false_positives_2: 15408.0000 - val_precision_2: 0.0047\n",
      "Epoch 111/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.1821 - false_positives_2: 54205.0000 - precision_2: 0.0025 - val_loss: 0.1827 - val_false_positives_2: 15202.0000 - val_precision_2: 0.0047\n",
      "Epoch 112/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.0131 - false_positives_2: 53393.0000 - precision_2: 0.0029 - val_loss: 0.1985 - val_false_positives_2: 16102.0000 - val_precision_2: 0.0046\n",
      "Epoch 113/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.9391 - false_positives_2: 55191.0000 - precision_2: 0.0032 - val_loss: 0.2211 - val_false_positives_2: 17105.0000 - val_precision_2: 0.0044\n",
      "Epoch 114/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.0015 - false_positives_2: 56028.0000 - precision_2: 0.0028 - val_loss: 0.2069 - val_false_positives_2: 16499.0000 - val_precision_2: 0.0045\n",
      "Epoch 115/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.1297 - false_positives_2: 54515.0000 - precision_2: 0.0025 - val_loss: 0.1919 - val_false_positives_2: 15780.0000 - val_precision_2: 0.0047\n",
      "Epoch 116/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9659 - false_positives_2: 53876.0000 - precision_2: 0.0033 - val_loss: 0.1907 - val_false_positives_2: 15722.0000 - val_precision_2: 0.0047\n",
      "Epoch 117/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.0095 - false_positives_2: 51406.0000 - precision_2: 0.0030 - val_loss: 0.1704 - val_false_positives_2: 14546.0000 - val_precision_2: 0.0047\n",
      "Epoch 118/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.0013 - false_positives_2: 53333.0000 - precision_2: 0.0028 - val_loss: 0.1996 - val_false_positives_2: 16224.0000 - val_precision_2: 0.0046\n",
      "Epoch 119/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.1082 - false_positives_2: 53825.0000 - precision_2: 0.0025 - val_loss: 0.1955 - val_false_positives_2: 16006.0000 - val_precision_2: 0.0047\n",
      "Epoch 120/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.0070 - false_positives_2: 53150.0000 - precision_2: 0.0027 - val_loss: 0.2216 - val_false_positives_2: 17148.0000 - val_precision_2: 0.0044\n",
      "Epoch 121/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.0306 - false_positives_2: 53527.0000 - precision_2: 0.0028 - val_loss: 0.2091 - val_false_positives_2: 16589.0000 - val_precision_2: 0.0045\n",
      "Epoch 122/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9808 - false_positives_2: 53347.0000 - precision_2: 0.0029 - val_loss: 0.2035 - val_false_positives_2: 16329.0000 - val_precision_2: 0.0046\n",
      "Epoch 123/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 1.0074 - false_positives_2: 52650.0000 - precision_2: 0.0031 - val_loss: 0.2120 - val_false_positives_2: 16705.0000 - val_precision_2: 0.0045\n",
      "Epoch 124/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9678 - false_positives_2: 53747.0000 - precision_2: 0.0032 - val_loss: 0.1971 - val_false_positives_2: 16034.0000 - val_precision_2: 0.0047\n",
      "Epoch 125/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.0099 - false_positives_2: 51818.0000 - precision_2: 0.0028 - val_loss: 0.2074 - val_false_positives_2: 16494.0000 - val_precision_2: 0.0045\n",
      "Epoch 126/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9355 - false_positives_2: 53585.0000 - precision_2: 0.0031 - val_loss: 0.1991 - val_false_positives_2: 16200.0000 - val_precision_2: 0.0046\n",
      "Epoch 127/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9802 - false_positives_2: 54309.0000 - precision_2: 0.0029 - val_loss: 0.2069 - val_false_positives_2: 16681.0000 - val_precision_2: 0.0045\n",
      "Epoch 128/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9683 - false_positives_2: 53481.0000 - precision_2: 0.0030 - val_loss: 0.2372 - val_false_positives_2: 17849.0000 - val_precision_2: 0.0043\n",
      "Epoch 129/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9272 - false_positives_2: 56166.0000 - precision_2: 0.0030 - val_loss: 0.2130 - val_false_positives_2: 16963.0000 - val_precision_2: 0.0045\n",
      "Epoch 130/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.0009 - false_positives_2: 51880.0000 - precision_2: 0.0030 - val_loss: 0.2383 - val_false_positives_2: 17853.0000 - val_precision_2: 0.0043\n",
      "Epoch 131/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9848 - false_positives_2: 53054.0000 - precision_2: 0.0030 - val_loss: 0.2212 - val_false_positives_2: 17287.0000 - val_precision_2: 0.0044\n",
      "Epoch 132/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9726 - false_positives_2: 53595.0000 - precision_2: 0.0029 - val_loss: 0.2199 - val_false_positives_2: 17253.0000 - val_precision_2: 0.0044\n",
      "Epoch 133/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9159 - false_positives_2: 52080.0000 - precision_2: 0.0031 - val_loss: 0.2138 - val_false_positives_2: 16993.0000 - val_precision_2: 0.0045\n",
      "Epoch 134/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.9068 - false_positives_2: 54282.0000 - precision_2: 0.0030 - val_loss: 0.2429 - val_false_positives_2: 18196.0000 - val_precision_2: 0.0042\n",
      "Epoch 135/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9028 - false_positives_2: 54575.0000 - precision_2: 0.0031 - val_loss: 0.2178 - val_false_positives_2: 17225.0000 - val_precision_2: 0.0044\n",
      "Epoch 136/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9778 - false_positives_2: 55251.0000 - precision_2: 0.0029 - val_loss: 0.2019 - val_false_positives_2: 16559.0000 - val_precision_2: 0.0045\n",
      "Epoch 137/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.9400 - false_positives_2: 54220.0000 - precision_2: 0.0028 - val_loss: 0.1914 - val_false_positives_2: 16033.0000 - val_precision_2: 0.0047\n",
      "Epoch 138/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.8374 - false_positives_2: 54398.0000 - precision_2: 0.0032 - val_loss: 0.2377 - val_false_positives_2: 18127.0000 - val_precision_2: 0.0042\n",
      "Epoch 139/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.8815 - false_positives_2: 53649.0000 - precision_2: 0.0030 - val_loss: 0.2016 - val_false_positives_2: 16571.0000 - val_precision_2: 0.0045\n",
      "Epoch 140/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9072 - false_positives_2: 52466.0000 - precision_2: 0.0030 - val_loss: 0.2116 - val_false_positives_2: 17035.0000 - val_precision_2: 0.0044\n",
      "Epoch 141/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9347 - false_positives_2: 54120.0000 - precision_2: 0.0028 - val_loss: 0.2561 - val_false_positives_2: 18714.0000 - val_precision_2: 0.0042\n",
      "Epoch 142/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.8943 - false_positives_2: 55222.0000 - precision_2: 0.0030 - val_loss: 0.2168 - val_false_positives_2: 17403.0000 - val_precision_2: 0.0043\n",
      "Epoch 143/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9015 - false_positives_2: 55559.0000 - precision_2: 0.0029 - val_loss: 0.2336 - val_false_positives_2: 18270.0000 - val_precision_2: 0.0043\n",
      "Epoch 144/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.8367 - false_positives_2: 54201.0000 - precision_2: 0.0031 - val_loss: 0.2108 - val_false_positives_2: 17243.0000 - val_precision_2: 0.0044\n",
      "Epoch 145/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.8850 - false_positives_2: 55901.0000 - precision_2: 0.0029 - val_loss: 0.2621 - val_false_positives_2: 19066.0000 - val_precision_2: 0.0041\n",
      "Epoch 146/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.8402 - false_positives_2: 54645.0000 - precision_2: 0.0032 - val_loss: 0.2357 - val_false_positives_2: 18132.0000 - val_precision_2: 0.0043\n",
      "Epoch 147/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.8000 - false_positives_2: 56594.0000 - precision_2: 0.0032 - val_loss: 0.2297 - val_false_positives_2: 17756.0000 - val_precision_2: 0.0044\n",
      "Epoch 148/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.8697 - false_positives_2: 55876.0000 - precision_2: 0.0028 - val_loss: 0.2186 - val_false_positives_2: 17501.0000 - val_precision_2: 0.0043\n",
      "Epoch 149/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9061 - false_positives_2: 55350.0000 - precision_2: 0.0030 - val_loss: 0.2184 - val_false_positives_2: 17523.0000 - val_precision_2: 0.0043\n",
      "Epoch 150/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.9195 - false_positives_2: 53339.0000 - precision_2: 0.0027 - val_loss: 0.2217 - val_false_positives_2: 17695.0000 - val_precision_2: 0.0043\n",
      "Epoch 151/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.8283 - false_positives_2: 52849.0000 - precision_2: 0.0032 - val_loss: 0.2271 - val_false_positives_2: 17948.0000 - val_precision_2: 0.0044\n",
      "Epoch 152/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.8702 - false_positives_2: 54422.0000 - precision_2: 0.0031 - val_loss: 0.2070 - val_false_positives_2: 16968.0000 - val_precision_2: 0.0045\n",
      "Epoch 153/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9598 - false_positives_2: 51994.0000 - precision_2: 0.0025 - val_loss: 0.1915 - val_false_positives_2: 15873.0000 - val_precision_2: 0.0047\n",
      "Epoch 154/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.8845 - false_positives_2: 50620.0000 - precision_2: 0.0028 - val_loss: 0.2280 - val_false_positives_2: 17862.0000 - val_precision_2: 0.0044\n",
      "Epoch 155/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.8552 - false_positives_2: 55815.0000 - precision_2: 0.0029 - val_loss: 0.2085 - val_false_positives_2: 16892.0000 - val_precision_2: 0.0045\n",
      "Epoch 156/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.8203 - false_positives_2: 53269.0000 - precision_2: 0.0030 - val_loss: 0.1993 - val_false_positives_2: 16408.0000 - val_precision_2: 0.0046\n",
      "Epoch 157/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.8308 - false_positives_2: 54344.0000 - precision_2: 0.0028 - val_loss: 0.2498 - val_false_positives_2: 18662.0000 - val_precision_2: 0.0042\n",
      "Epoch 158/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7957 - false_positives_2: 55243.0000 - precision_2: 0.0032 - val_loss: 0.2052 - val_false_positives_2: 16753.0000 - val_precision_2: 0.0045\n",
      "Epoch 159/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7936 - false_positives_2: 53946.0000 - precision_2: 0.0031 - val_loss: 0.3279 - val_false_positives_2: 21140.0000 - val_precision_2: 0.0037\n",
      "Epoch 160/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.8110 - false_positives_2: 55301.0000 - precision_2: 0.0031 - val_loss: 0.1877 - val_false_positives_2: 15024.0000 - val_precision_2: 0.0047\n",
      "Epoch 161/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.7611 - false_positives_2: 54364.0000 - precision_2: 0.0032 - val_loss: 0.2461 - val_false_positives_2: 18831.0000 - val_precision_2: 0.0042\n",
      "Epoch 162/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7945 - false_positives_2: 55424.0000 - precision_2: 0.0030 - val_loss: 0.1939 - val_false_positives_2: 15704.0000 - val_precision_2: 0.0048\n",
      "Epoch 163/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7778 - false_positives_2: 53954.0000 - precision_2: 0.0032 - val_loss: 0.1785 - val_false_positives_2: 13745.0000 - val_precision_2: 0.0046\n",
      "Epoch 164/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7278 - false_positives_2: 54714.0000 - precision_2: 0.0033 - val_loss: 0.2883 - val_false_positives_2: 20056.0000 - val_precision_2: 0.0039\n",
      "Epoch 165/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.8057 - false_positives_2: 53940.0000 - precision_2: 0.0030 - val_loss: 0.2702 - val_false_positives_2: 19485.0000 - val_precision_2: 0.0040\n",
      "Epoch 166/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7723 - false_positives_2: 55727.0000 - precision_2: 0.0031 - val_loss: 0.2870 - val_false_positives_2: 20178.0000 - val_precision_2: 0.0039\n",
      "Epoch 167/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.7610 - false_positives_2: 54898.0000 - precision_2: 0.0031 - val_loss: 0.2197 - val_false_positives_2: 17421.0000 - val_precision_2: 0.0044\n",
      "Epoch 168/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7613 - false_positives_2: 54631.0000 - precision_2: 0.0031 - val_loss: 0.2042 - val_false_positives_2: 16296.0000 - val_precision_2: 0.0046\n",
      "Epoch 169/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.8021 - false_positives_2: 52698.0000 - precision_2: 0.0030 - val_loss: 0.2014 - val_false_positives_2: 15973.0000 - val_precision_2: 0.0047\n",
      "Epoch 170/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.8127 - false_positives_2: 53636.0000 - precision_2: 0.0028 - val_loss: 0.2033 - val_false_positives_2: 16044.0000 - val_precision_2: 0.0047\n",
      "Epoch 171/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7603 - false_positives_2: 52856.0000 - precision_2: 0.0031 - val_loss: 0.2195 - val_false_positives_2: 17238.0000 - val_precision_2: 0.0044\n",
      "Epoch 172/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.7847 - false_positives_2: 53017.0000 - precision_2: 0.0030 - val_loss: 0.2901 - val_false_positives_2: 20348.0000 - val_precision_2: 0.0039\n",
      "Epoch 173/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.7708 - false_positives_2: 53461.0000 - precision_2: 0.0029 - val_loss: 0.2309 - val_false_positives_2: 17923.0000 - val_precision_2: 0.0044\n",
      "Epoch 174/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7239 - false_positives_2: 53548.0000 - precision_2: 0.0029 - val_loss: 0.2520 - val_false_positives_2: 19002.0000 - val_precision_2: 0.0041\n",
      "Epoch 175/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7709 - false_positives_2: 55534.0000 - precision_2: 0.0029 - val_loss: 0.2538 - val_false_positives_2: 19209.0000 - val_precision_2: 0.0041\n",
      "Epoch 176/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7663 - false_positives_2: 54721.0000 - precision_2: 0.0028 - val_loss: 0.2241 - val_false_positives_2: 17563.0000 - val_precision_2: 0.0044\n",
      "Epoch 177/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.6990 - false_positives_2: 54680.0000 - precision_2: 0.0032 - val_loss: 0.1937 - val_false_positives_2: 10348.0000 - val_precision_2: 0.0042\n",
      "Epoch 178/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7351 - false_positives_2: 56890.0000 - precision_2: 0.0030 - val_loss: 0.2524 - val_false_positives_2: 18919.0000 - val_precision_2: 0.0042\n",
      "Epoch 179/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7381 - false_positives_2: 53208.0000 - precision_2: 0.0028 - val_loss: 0.2440 - val_false_positives_2: 18477.0000 - val_precision_2: 0.0043\n",
      "Epoch 180/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.7362 - false_positives_2: 53443.0000 - precision_2: 0.0029 - val_loss: 0.2120 - val_false_positives_2: 15436.0000 - val_precision_2: 0.0048\n",
      "Epoch 181/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7280 - false_positives_2: 55228.0000 - precision_2: 0.0031 - val_loss: 0.2471 - val_false_positives_2: 18757.0000 - val_precision_2: 0.0042\n",
      "Epoch 182/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6598 - false_positives_2: 55413.0000 - precision_2: 0.0033 - val_loss: 0.2643 - val_false_positives_2: 19606.0000 - val_precision_2: 0.0040\n",
      "Epoch 183/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7280 - false_positives_2: 53789.0000 - precision_2: 0.0030 - val_loss: 0.2461 - val_false_positives_2: 18530.0000 - val_precision_2: 0.0042\n",
      "Epoch 184/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7525 - false_positives_2: 52520.0000 - precision_2: 0.0028 - val_loss: 0.2902 - val_false_positives_2: 20572.0000 - val_precision_2: 0.0038\n",
      "Epoch 185/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.7262 - false_positives_2: 54545.0000 - precision_2: 0.0029 - val_loss: 0.2598 - val_false_positives_2: 19185.0000 - val_precision_2: 0.0041\n",
      "Epoch 186/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7231 - false_positives_2: 56590.0000 - precision_2: 0.0029 - val_loss: 0.2782 - val_false_positives_2: 20067.0000 - val_precision_2: 0.0039\n",
      "Epoch 187/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6800 - false_positives_2: 54206.0000 - precision_2: 0.0032 - val_loss: 0.2144 - val_false_positives_2: 14889.0000 - val_precision_2: 0.0047\n",
      "Epoch 188/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.6764 - false_positives_2: 54200.0000 - precision_2: 0.0032 - val_loss: 0.2476 - val_false_positives_2: 18680.0000 - val_precision_2: 0.0042\n",
      "Epoch 189/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7149 - false_positives_2: 55392.0000 - precision_2: 0.0030 - val_loss: 0.3020 - val_false_positives_2: 21242.0000 - val_precision_2: 0.0037\n",
      "Epoch 190/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7293 - false_positives_2: 54727.0000 - precision_2: 0.0027 - val_loss: 0.4382 - val_false_positives_2: 24155.0000 - val_precision_2: 0.0034\n",
      "Epoch 191/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6659 - false_positives_2: 53580.0000 - precision_2: 0.0032 - val_loss: 0.2790 - val_false_positives_2: 20319.0000 - val_precision_2: 0.0039\n",
      "Epoch 192/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6757 - false_positives_2: 52564.0000 - precision_2: 0.0032 - val_loss: 0.2147 - val_false_positives_2: 11770.0000 - val_precision_2: 0.0044\n",
      "Epoch 193/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6946 - false_positives_2: 54895.0000 - precision_2: 0.0031 - val_loss: 0.2828 - val_false_positives_2: 20560.0000 - val_precision_2: 0.0038\n",
      "Epoch 194/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.7208 - false_positives_2: 55884.0000 - precision_2: 0.0029 - val_loss: 0.2186 - val_false_positives_2: 11608.0000 - val_precision_2: 0.0045\n",
      "Epoch 195/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6711 - false_positives_2: 54546.0000 - precision_2: 0.0031 - val_loss: 0.3340 - val_false_positives_2: 22429.0000 - val_precision_2: 0.0036\n",
      "Epoch 196/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.6741 - false_positives_2: 54461.0000 - precision_2: 0.0030 - val_loss: 0.3657 - val_false_positives_2: 23151.0000 - val_precision_2: 0.0035\n",
      "Epoch 197/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7194 - false_positives_2: 56860.0000 - precision_2: 0.0028 - val_loss: 0.2735 - val_false_positives_2: 19880.0000 - val_precision_2: 0.0040\n",
      "Epoch 198/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6221 - false_positives_2: 53792.0000 - precision_2: 0.0033 - val_loss: 0.3193 - val_false_positives_2: 21969.0000 - val_precision_2: 0.0037\n",
      "Epoch 199/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6218 - false_positives_2: 53883.0000 - precision_2: 0.0032 - val_loss: 0.4379 - val_false_positives_2: 24498.0000 - val_precision_2: 0.0034\n",
      "Epoch 200/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7018 - false_positives_2: 55351.0000 - precision_2: 0.0028 - val_loss: 0.2782 - val_false_positives_2: 20078.0000 - val_precision_2: 0.0039\n",
      "Epoch 201/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.6541 - false_positives_2: 54255.0000 - precision_2: 0.0029 - val_loss: 0.2363 - val_false_positives_2: 8017.0000 - val_precision_2: 0.0029\n",
      "Epoch 202/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7090 - false_positives_2: 56162.0000 - precision_2: 0.0032 - val_loss: 0.2731 - val_false_positives_2: 19759.0000 - val_precision_2: 0.0040\n",
      "Epoch 203/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6185 - false_positives_2: 54389.0000 - precision_2: 0.0031 - val_loss: 0.2514 - val_false_positives_2: 17378.0000 - val_precision_2: 0.0045\n",
      "Epoch 204/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.6354 - false_positives_2: 55522.0000 - precision_2: 0.0031 - val_loss: 0.2601 - val_false_positives_2: 18465.0000 - val_precision_2: 0.0043\n",
      "Epoch 205/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.6277 - false_positives_2: 54499.0000 - precision_2: 0.0032 - val_loss: 0.2830 - val_false_positives_2: 20222.0000 - val_precision_2: 0.0039\n",
      "Epoch 206/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6423 - false_positives_2: 57681.0000 - precision_2: 0.0031 - val_loss: 0.6690 - val_false_positives_2: 26697.0000 - val_precision_2: 0.0031\n",
      "Epoch 207/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.6396 - false_positives_2: 55536.0000 - precision_2: 0.0030 - val_loss: 0.3058 - val_false_positives_2: 21438.0000 - val_precision_2: 0.0037\n",
      "Epoch 208/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.6543 - false_positives_2: 54518.0000 - precision_2: 0.0027 - val_loss: 0.4863 - val_false_positives_2: 25303.0000 - val_precision_2: 0.0033\n",
      "Epoch 209/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6824 - false_positives_2: 58862.0000 - precision_2: 0.0029 - val_loss: 0.6507 - val_false_positives_2: 26493.0000 - val_precision_2: 0.0032\n",
      "Epoch 210/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7623 - false_positives_2: 59927.0000 - precision_2: 0.0027 - val_loss: 0.2568 - val_false_positives_2: 408.0000 - val_precision_2: 0.0000e+00\n",
      "Epoch 211/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6926 - false_positives_2: 58412.0000 - precision_2: 0.0030 - val_loss: 0.5252 - val_false_positives_2: 26031.0000 - val_precision_2: 0.0032\n",
      "Epoch 212/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6064 - false_positives_2: 53514.0000 - precision_2: 0.0032 - val_loss: 0.3268 - val_false_positives_2: 22660.0000 - val_precision_2: 0.0036\n",
      "Epoch 213/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6589 - false_positives_2: 56917.0000 - precision_2: 0.0028 - val_loss: 0.2857 - val_false_positives_2: 20164.0000 - val_precision_2: 0.0039\n",
      "Epoch 214/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6370 - false_positives_2: 54998.0000 - precision_2: 0.0030 - val_loss: 0.2850 - val_false_positives_2: 19822.0000 - val_precision_2: 0.0040\n",
      "Epoch 215/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.5799 - false_positives_2: 54313.0000 - precision_2: 0.0034 - val_loss: 0.3190 - val_false_positives_2: 22211.0000 - val_precision_2: 0.0036\n",
      "Epoch 216/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.5873 - false_positives_2: 57338.0000 - precision_2: 0.0033 - val_loss: 0.2838 - val_false_positives_2: 19017.0000 - val_precision_2: 0.0041\n",
      "Epoch 217/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6269 - false_positives_2: 57686.0000 - precision_2: 0.0030 - val_loss: 0.5004 - val_false_positives_2: 26133.0000 - val_precision_2: 0.0032\n",
      "Epoch 218/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.6237 - false_positives_2: 54598.0000 - precision_2: 0.0029 - val_loss: 0.3658 - val_false_positives_2: 23836.0000 - val_precision_2: 0.0034\n",
      "Epoch 219/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6428 - false_positives_2: 57941.0000 - precision_2: 0.0030 - val_loss: 1.3125 - val_false_positives_2: 28972.0000 - val_precision_2: 0.0029\n",
      "Epoch 220/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7658 - false_positives_2: 64261.0000 - precision_2: 0.0027 - val_loss: 0.2827 - val_false_positives_2: 17676.0000 - val_precision_2: 0.0044\n",
      "Epoch 221/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.6166 - false_positives_2: 55743.0000 - precision_2: 0.0033 - val_loss: 0.2820 - val_false_positives_2: 16783.0000 - val_precision_2: 0.0046\n",
      "Epoch 222/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.6257 - false_positives_2: 54139.0000 - precision_2: 0.0029 - val_loss: 0.3130 - val_false_positives_2: 21162.0000 - val_precision_2: 0.0037\n",
      "Epoch 223/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.5698 - false_positives_2: 55842.0000 - precision_2: 0.0035 - val_loss: 0.5967 - val_false_positives_2: 26583.0000 - val_precision_2: 0.0031\n",
      "Epoch 224/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.6535 - false_positives_2: 57727.0000 - precision_2: 0.0030 - val_loss: 0.3095 - val_false_positives_2: 20522.0000 - val_precision_2: 0.0038\n",
      "Epoch 225/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6189 - false_positives_2: 57371.0000 - precision_2: 0.0030 - val_loss: 0.3782 - val_false_positives_2: 23955.0000 - val_precision_2: 0.0035\n",
      "Epoch 226/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.5748 - false_positives_2: 56448.0000 - precision_2: 0.0032 - val_loss: 0.3046 - val_false_positives_2: 14088.0000 - val_precision_2: 0.0047\n",
      "Epoch 227/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6978 - false_positives_2: 59386.0000 - precision_2: 0.0027 - val_loss: 1.1211 - val_false_positives_2: 28124.0000 - val_precision_2: 0.0030\n",
      "Epoch 228/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.6461 - false_positives_2: 55412.0000 - precision_2: 0.0032 - val_loss: 0.6711 - val_false_positives_2: 26852.0000 - val_precision_2: 0.0031\n",
      "Epoch 229/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6539 - false_positives_2: 57963.0000 - precision_2: 0.0031 - val_loss: 0.9405 - val_false_positives_2: 27884.0000 - val_precision_2: 0.0030\n",
      "Epoch 230/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9818 - false_positives_2: 69448.0000 - precision_2: 0.0028 - val_loss: 0.3553 - val_false_positives_2: 23216.0000 - val_precision_2: 0.0035\n",
      "Epoch 231/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6266 - false_positives_2: 57762.0000 - precision_2: 0.0029 - val_loss: 0.3373 - val_false_positives_2: 22398.0000 - val_precision_2: 0.0036\n",
      "Epoch 232/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.5890 - false_positives_2: 56785.0000 - precision_2: 0.0032 - val_loss: 0.4059 - val_false_positives_2: 25085.0000 - val_precision_2: 0.0033\n",
      "Epoch 233/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.5724 - false_positives_2: 57334.0000 - precision_2: 0.0031 - val_loss: 0.4905 - val_false_positives_2: 26325.0000 - val_precision_2: 0.0032\n",
      "Epoch 234/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 1.1518 - false_positives_2: 72449.0000 - precision_2: 0.0025 - val_loss: 0.7924 - val_false_positives_2: 28307.0000 - val_precision_2: 0.0030\n",
      "Epoch 235/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6651 - false_positives_2: 64829.0000 - precision_2: 0.0027 - val_loss: 0.3826 - val_false_positives_2: 24618.0000 - val_precision_2: 0.0034\n",
      "Epoch 236/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.5361 - false_positives_2: 55014.0000 - precision_2: 0.0033 - val_loss: 0.3423 - val_false_positives_2: 22886.0000 - val_precision_2: 0.0035\n",
      "Epoch 237/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.5490 - false_positives_2: 58486.0000 - precision_2: 0.0032 - val_loss: 0.5019 - val_false_positives_2: 27021.0000 - val_precision_2: 0.0031\n",
      "Epoch 238/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.6648 - false_positives_2: 63730.0000 - precision_2: 0.0031 - val_loss: 0.3504 - val_false_positives_2: 23341.0000 - val_precision_2: 0.0035\n",
      "Epoch 239/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.5715 - false_positives_2: 61735.0000 - precision_2: 0.0030 - val_loss: 0.4083 - val_false_positives_2: 26125.0000 - val_precision_2: 0.0032\n",
      "Epoch 240/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.8041 - false_positives_2: 67502.0000 - precision_2: 0.0029 - val_loss: 0.3482 - val_false_positives_2: 21564.0000 - val_precision_2: 0.0037\n",
      "Epoch 241/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.6237 - false_positives_2: 63817.0000 - precision_2: 0.0028 - val_loss: 0.5811 - val_false_positives_2: 28206.0000 - val_precision_2: 0.0030\n",
      "Epoch 242/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.6318 - false_positives_2: 64187.0000 - precision_2: 0.0029 - val_loss: 0.4288 - val_false_positives_2: 0.0000e+00 - val_precision_2: 0.0000e+00\n",
      "Epoch 243/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.9320 - false_positives_2: 73279.0000 - precision_2: 0.0028 - val_loss: 0.4892 - val_false_positives_2: 27240.0000 - val_precision_2: 0.0031\n",
      "Epoch 244/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.5821 - false_positives_2: 65379.0000 - precision_2: 0.0030 - val_loss: 0.3540 - val_false_positives_2: 22486.0000 - val_precision_2: 0.0036\n",
      "Epoch 245/250\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.5678 - false_positives_2: 59042.0000 - precision_2: 0.0028 - val_loss: 0.4351 - val_false_positives_2: 26397.0000 - val_precision_2: 0.0032\n",
      "Epoch 246/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.5982 - false_positives_2: 61792.0000 - precision_2: 0.0027 - val_loss: 0.3643 - val_false_positives_2: 16417.0000 - val_precision_2: 0.0046\n",
      "Epoch 247/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.5539 - false_positives_2: 60176.0000 - precision_2: 0.0033 - val_loss: 0.3625 - val_false_positives_2: 21424.0000 - val_precision_2: 0.0037\n",
      "Epoch 248/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.5522 - false_positives_2: 61517.0000 - precision_2: 0.0030 - val_loss: 0.4185 - val_false_positives_2: 25993.0000 - val_precision_2: 0.0032\n",
      "Epoch 249/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.5837 - false_positives_2: 63712.0000 - precision_2: 0.0030 - val_loss: 0.4708 - val_false_positives_2: 27027.0000 - val_precision_2: 0.0031\n",
      "Epoch 250/250\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.5728 - false_positives_2: 61612.0000 - precision_2: 0.0028 - val_loss: 0.3844 - val_false_positives_2: 23702.0000 - val_precision_2: 0.0034\n"
     ]
    }
   ],
   "source": [
    "weighted_model = make_model()\n",
    "weighted_model.load_weights(initial_weights)\n",
    "EPOCHS = 256\n",
    "weighted_history = weighted_model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(val_features, val_labels),\n",
    "    # The class weights go here\n",
    "    class_weight=class_weight,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 15:07:31.331425: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 1s 5ms/step\n",
      "12/12 [==============================] - 0s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "train_predictions_weighted = weighted_model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_weighted = weighted_model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 7ms/step - loss: 0.3848 - false_positives_2: 29716.0000 - precision_2: 0.0034\n",
      "loss :  0.3847528398036957\n",
      "false_positives_2 :  29716.0\n",
      "precision_2 :  0.003387329401448369\n",
      "\n",
      "Legitimate Transactions Detected (True Negatives):  169705\n",
      "Legitimate Transactions Incorrectly Detected (False Positives):  17687\n",
      "Fraudulent Transactions Missed (False Negatives):  26\n",
      "Fraudulent Transactions Detected (True Positives):  82\n",
      "Total Fraudulent Transactions:  108\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weighted_results = weighted_model.evaluate(test_features, test_labels,\n",
    "                                           batch_size=BATCH_SIZE, verbose=1)\n",
    "for name, value in zip(weighted_model.metrics_names, weighted_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(test_labels, test_predictions_weighted,threshold=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_mac",
   "language": "python",
   "name": "tf_mac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
