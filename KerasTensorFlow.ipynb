{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras.layers import Flatten\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Coordinates</th>\n",
       "      <th>nearest_time</th>\n",
       "      <th>lightning</th>\n",
       "      <th>CMI_C01</th>\n",
       "      <th>CMI_C02</th>\n",
       "      <th>CMI_C03</th>\n",
       "      <th>CMI_C04</th>\n",
       "      <th>CMI_C05</th>\n",
       "      <th>CMI_C06</th>\n",
       "      <th>...</th>\n",
       "      <th>CMI_C15</th>\n",
       "      <th>CMI_C16</th>\n",
       "      <th>ACM</th>\n",
       "      <th>BCM</th>\n",
       "      <th>Cloud_Probabilities</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>time</th>\n",
       "      <th>time_int</th>\n",
       "      <th>Lightning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(40.13103323474366, -93.38155072424266)</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.331051</td>\n",
       "      <td>0.283293</td>\n",
       "      <td>0.454821</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.371012</td>\n",
       "      <td>0.296587</td>\n",
       "      <td>...</td>\n",
       "      <td>274.79310</td>\n",
       "      <td>263.45618</td>\n",
       "      <td>1.734227</td>\n",
       "      <td>0.734227</td>\n",
       "      <td>0.505974</td>\n",
       "      <td>40.131033</td>\n",
       "      <td>-93.381551</td>\n",
       "      <td>2022-05-21 19:00:31.268224</td>\n",
       "      <td>1653159631268224000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>(40.12698592712501, -93.2741436595977)</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.296647</td>\n",
       "      <td>0.242758</td>\n",
       "      <td>0.444345</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.330734</td>\n",
       "      <td>0.259226</td>\n",
       "      <td>...</td>\n",
       "      <td>275.39243</td>\n",
       "      <td>263.77637</td>\n",
       "      <td>1.375754</td>\n",
       "      <td>0.500453</td>\n",
       "      <td>0.446567</td>\n",
       "      <td>40.126986</td>\n",
       "      <td>-93.274144</td>\n",
       "      <td>2022-05-21 19:00:31.268224</td>\n",
       "      <td>1653159631268224000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>(40.122965705716815, -93.16685328500078)</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.301825</td>\n",
       "      <td>0.247361</td>\n",
       "      <td>0.450753</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.329881</td>\n",
       "      <td>0.260476</td>\n",
       "      <td>...</td>\n",
       "      <td>275.28073</td>\n",
       "      <td>263.76944</td>\n",
       "      <td>1.593902</td>\n",
       "      <td>0.593902</td>\n",
       "      <td>0.484379</td>\n",
       "      <td>40.122966</td>\n",
       "      <td>-93.166853</td>\n",
       "      <td>2022-05-21 19:00:31.268224</td>\n",
       "      <td>1653159631268224000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>(40.11897247748837, -93.05967862352706)</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344345</td>\n",
       "      <td>0.293075</td>\n",
       "      <td>0.490972</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>0.365119</td>\n",
       "      <td>0.283512</td>\n",
       "      <td>...</td>\n",
       "      <td>275.14670</td>\n",
       "      <td>263.74536</td>\n",
       "      <td>1.828127</td>\n",
       "      <td>0.828127</td>\n",
       "      <td>0.638011</td>\n",
       "      <td>40.118972</td>\n",
       "      <td>-93.059679</td>\n",
       "      <td>2022-05-21 19:00:31.268224</td>\n",
       "      <td>1653159631268224000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>(40.115006150366405, -92.95261870531907)</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.335952</td>\n",
       "      <td>0.285119</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.361805</td>\n",
       "      <td>0.283472</td>\n",
       "      <td>...</td>\n",
       "      <td>275.35516</td>\n",
       "      <td>263.82800</td>\n",
       "      <td>1.796349</td>\n",
       "      <td>0.811898</td>\n",
       "      <td>0.613374</td>\n",
       "      <td>40.115006</td>\n",
       "      <td>-92.952619</td>\n",
       "      <td>2022-05-21 19:00:31.268224</td>\n",
       "      <td>1653159631268224000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                               Coordinates nearest_time  \\\n",
       "0           0   (40.13103323474366, -93.38155072424266)            0   \n",
       "1           1    (40.12698592712501, -93.2741436595977)            0   \n",
       "2           2  (40.122965705716815, -93.16685328500078)            0   \n",
       "3           3   (40.11897247748837, -93.05967862352706)            0   \n",
       "4           4  (40.115006150366405, -92.95261870531907)            0   \n",
       "\n",
       "   lightning   CMI_C01   CMI_C02   CMI_C03   CMI_C04   CMI_C05   CMI_C06  ...  \\\n",
       "0        0.0  0.331051  0.283293  0.454821  0.001786  0.371012  0.296587  ...   \n",
       "1        0.0  0.296647  0.242758  0.444345  0.001746  0.330734  0.259226  ...   \n",
       "2        0.0  0.301825  0.247361  0.450753  0.001825  0.329881  0.260476  ...   \n",
       "3        0.0  0.344345  0.293075  0.490972  0.002063  0.365119  0.283512  ...   \n",
       "4        0.0  0.335952  0.285119  0.485000  0.002103  0.361805  0.283472  ...   \n",
       "\n",
       "     CMI_C15    CMI_C16       ACM       BCM  Cloud_Probabilities        lat  \\\n",
       "0  274.79310  263.45618  1.734227  0.734227             0.505974  40.131033   \n",
       "1  275.39243  263.77637  1.375754  0.500453             0.446567  40.126986   \n",
       "2  275.28073  263.76944  1.593902  0.593902             0.484379  40.122966   \n",
       "3  275.14670  263.74536  1.828127  0.828127             0.638011  40.118972   \n",
       "4  275.35516  263.82800  1.796349  0.811898             0.613374  40.115006   \n",
       "\n",
       "         lon                        time             time_int  Lightning  \n",
       "0 -93.381551  2022-05-21 19:00:31.268224  1653159631268224000          0  \n",
       "1 -93.274144  2022-05-21 19:00:31.268224  1653159631268224000          0  \n",
       "2 -93.166853  2022-05-21 19:00:31.268224  1653159631268224000          0  \n",
       "3 -93.059679  2022-05-21 19:00:31.268224  1653159631268224000          0  \n",
       "4 -92.952619  2022-05-21 19:00:31.268224  1653159631268224000          0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/robbiefeldstein/Documents/Programming/Research/Datasets/group_May_22.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "    Total: 468750\n",
      "    Positive: 25562 (5.45% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Look at class imbalance\n",
    "\n",
    "neg, pos = np.bincount(df['Lightning'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"CMI_C01\", \"CMI_C02\", \"CMI_C03\",\"CMI_C04\", \"CMI_C05\",\"CMI_C06\", \"CMI_C07\",\"CMI_C15\",\"Cloud_Probabilities\",\"Lightning\"]\n",
    "#let's just do less features\n",
    "#Predictors\n",
    "\n",
    "copy_df = df.copy()\n",
    "copy_df = copy_df[features]\n",
    "\n",
    "X = copy_df[features]\n",
    "\n",
    "# Use a utility from sklearn to split and shuffle your dataset.\n",
    "train_df, test_df = train_test_split(copy_df, test_size=0.3)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.3)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('Lightning'))\n",
    "bool_train_labels = train_labels != 0\n",
    "val_labels = np.array(val_df.pop('Lightning'))\n",
    "test_labels = np.array(test_df.pop('Lightning'))\n",
    "\n",
    "train_features = np.array(train_df)\n",
    "val_features = np.array(val_df)\n",
    "test_features = np.array(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average class probability in training set:   0.0544\n",
      "Average class probability in validation set: 0.0549\n",
      "Average class probability in test set:       0.0544\n"
     ]
    }
   ],
   "source": [
    "#Averages are roughly similar\n",
    "\n",
    "print(f'Average class probability in training set:   {train_labels.mean():.4f}')\n",
    "print(f'Average class probability in validation set: {val_labels.mean():.4f}')\n",
    "print(f'Average class probability in test set:       {test_labels.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (229687,)\n",
      "Validation labels shape: (98438,)\n",
      "Test labels shape: (140625,)\n",
      "Training features shape: (229687, 9)\n",
      "Validation features shape: (98438, 9)\n",
      "Test features shape: (140625, 9)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "train_features = np.clip(train_features, -5, 5)\n",
    "val_features = np.clip(val_features, -5, 5)\n",
    "test_features = np.clip(test_features, -5, 5)\n",
    "\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recommended parameters for imbalanced model\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.BinaryCrossentropy(name='cross entropy'),  # same as model's loss\n",
    "      keras.metrics.MeanSquaredError(name='Brier score'),\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "def make_model(metrics=METRICS, output_bias=None):\n",
    "  if output_bias is not None:\n",
    "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "\n",
    "  model = keras.Sequential([\n",
    "    keras.layers.Dense(len(features), activation='relu', input_shape=(train_features.shape[-1],)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(4, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(2, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    #Output layer\n",
    "    keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)\n",
    "])\n",
    "\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.legacy.Adam(learning_rate=1e-3),\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=[tf.keras.metrics.Precision()])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 128\n",
    "BATCH_SIZE = 16384\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.9089627]\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_36 (Dense)            (None, 10)                100       \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 10)                40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 16)                176       \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 2)                 10        \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 501 (1.96 KB)\n",
      "Trainable params: 481 (1.88 KB)\n",
      "Non-trainable params: 20 (80.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initial_bias = np.log([pos/len(df)])\n",
    "print(initial_bias)\n",
    "model = make_model(output_bias=initial_bias)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\n",
    "model.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 15:24:12.589721: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 15:24:14.787992: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 312ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.4899383e-03],\n",
       "       [1.1355537e-01],\n",
       "       [2.5141600e-01],\n",
       "       [4.6908468e-04],\n",
       "       [8.6929533e-04],\n",
       "       [6.8165445e-01],\n",
       "       [9.6954399e-01],\n",
       "       [4.6863263e-03],\n",
       "       [4.6336444e-04],\n",
       "       [1.8483832e-02]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "print(\"Loss: {:0.4f}\".format(results[0]))\n",
    "\n",
    "model = make_model(output_bias=initial_bias)\n",
    "model.predict(train_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = make_model()\n",
    "# model.load_weights(initial_weights)\n",
    "# model.layers[-1].bias.assign([0.0])\n",
    "# zero_bias_history = model.fit(\n",
    "#     train_features,\n",
    "#     train_labels,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     epochs=16,\n",
    "#     validation_data=(val_features, val_labels), \n",
    "#     verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = make_model()\n",
    "# model.load_weights(initial_weights)\n",
    "# careful_bias_history = model.fit(\n",
    "#     train_features,\n",
    "#     train_labels,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     epochs=16,\n",
    "#     validation_data=(val_features, val_labels), \n",
    "#     verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, label, n):\n",
    "  # Use a log scale on y-axis to show the wide range of values.\n",
    "  plt.semilogy(history.epoch, history.history['loss'],\n",
    "               color=colors[n], label='Train ' + label)\n",
    "  plt.semilogy(history.epoch, history.history['val_loss'],\n",
    "               color=colors[n], label='Val ' + label,\n",
    "               linestyle=\"--\")\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_loss(zero_bias_history, \"Zero Bias\", 0)\n",
    "# plot_loss(careful_bias_history, \"Careful Bias\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "  metrics = ['loss', 'precision',]\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0, plt.ylim()[1]])\n",
    "    elif metric == 'auc':\n",
    "      plt.ylim([0.8,1])\n",
    "    else:\n",
    "      plt.ylim([0,1])\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, threshold=0.5):\n",
    "  cm = confusion_matrix(labels, predictions > threshold)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(threshold))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class 0: 0.53\n",
      "Weight for class 1: 9.17\n"
     ]
    }
   ],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 15:24:15.884684: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - ETA: 0s - loss: 2.1058 - precision_8: 0.0649"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 15:24:23.694169: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 10s 396ms/step - loss: 2.1058 - precision_8: 0.0649 - val_loss: 0.2008 - val_precision_8: 0.0000e+00\n",
      "Epoch 2/250\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 1.8750 - precision_8: 0.0777 - val_loss: 0.1851 - val_precision_8: 0.0000e+00\n",
      "Epoch 3/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.7557 - precision_8: 0.0865 - val_loss: 0.1866 - val_precision_8: 0.0000e+00\n",
      "Epoch 4/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.7310 - precision_8: 0.0914 - val_loss: 0.1917 - val_precision_8: 0.6667\n",
      "Epoch 5/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.6703 - precision_8: 0.0966 - val_loss: 0.1963 - val_precision_8: 0.5578\n",
      "Epoch 6/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.6136 - precision_8: 0.1005 - val_loss: 0.2051 - val_precision_8: 0.4118\n",
      "Epoch 7/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.5937 - precision_8: 0.1025 - val_loss: 0.2161 - val_precision_8: 0.3200\n",
      "Epoch 8/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.5716 - precision_8: 0.1036 - val_loss: 0.2261 - val_precision_8: 0.2828\n",
      "Epoch 9/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.5736 - precision_8: 0.1037 - val_loss: 0.2275 - val_precision_8: 0.2783\n",
      "Epoch 10/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.5375 - precision_8: 0.1061 - val_loss: 0.2230 - val_precision_8: 0.2946\n",
      "Epoch 11/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.5269 - precision_8: 0.1017 - val_loss: 0.2232 - val_precision_8: 0.2945\n",
      "Epoch 12/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.4879 - precision_8: 0.1037 - val_loss: 0.2236 - val_precision_8: 0.2964\n",
      "Epoch 13/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.4712 - precision_8: 0.1045 - val_loss: 0.2238 - val_precision_8: 0.3028\n",
      "Epoch 14/250\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 1.4753 - precision_8: 0.1026 - val_loss: 0.2191 - val_precision_8: 0.3362\n",
      "Epoch 15/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.4408 - precision_8: 0.1033 - val_loss: 0.2166 - val_precision_8: 0.3693\n",
      "Epoch 16/250\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 1.4380 - precision_8: 0.1009 - val_loss: 0.2166 - val_precision_8: 0.3735\n",
      "Epoch 17/250\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 1.4260 - precision_8: 0.1026 - val_loss: 0.2131 - val_precision_8: 0.4382\n",
      "Epoch 18/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.3946 - precision_8: 0.1016 - val_loss: 0.2089 - val_precision_8: 0.6000\n",
      "Epoch 19/250\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 1.3748 - precision_8: 0.1024 - val_loss: 0.2103 - val_precision_8: 0.0000e+00\n",
      "Epoch 20/250\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 1.3754 - precision_8: 0.1013 - val_loss: 0.2095 - val_precision_8: 0.0000e+00\n",
      "Epoch 21/250\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 1.3572 - precision_8: 0.1020 - val_loss: 0.2085 - val_precision_8: 0.0000e+00\n",
      "Epoch 22/250\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 1.3476 - precision_8: 0.0986 - val_loss: 0.2092 - val_precision_8: 0.0000e+00\n",
      "Epoch 23/250\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 1.3203 - precision_8: 0.1002 - val_loss: 0.2099 - val_precision_8: 0.0000e+00\n",
      "Epoch 24/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.3170 - precision_8: 0.0999 - val_loss: 0.2094 - val_precision_8: 0.0000e+00\n",
      "Epoch 25/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.3038 - precision_8: 0.0988 - val_loss: 0.2099 - val_precision_8: 0.0000e+00\n",
      "Epoch 26/250\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.2930 - precision_8: 0.0980 - val_loss: 0.2095 - val_precision_8: 0.0000e+00\n",
      "Epoch 27/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.2610 - precision_8: 0.0983 - val_loss: 0.2087 - val_precision_8: 0.0000e+00\n",
      "Epoch 28/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.2509 - precision_8: 0.0970 - val_loss: 0.2095 - val_precision_8: 0.0000e+00\n",
      "Epoch 29/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.2500 - precision_8: 0.0941 - val_loss: 0.2097 - val_precision_8: 0.0000e+00\n",
      "Epoch 30/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.2360 - precision_8: 0.0955 - val_loss: 0.2109 - val_precision_8: 0.0000e+00\n",
      "Epoch 31/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.2238 - precision_8: 0.0931 - val_loss: 0.2123 - val_precision_8: 0.0000e+00\n",
      "Epoch 32/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.2172 - precision_8: 0.0909 - val_loss: 0.2126 - val_precision_8: 0.0000e+00\n",
      "Epoch 33/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.1832 - precision_8: 0.0931 - val_loss: 0.2162 - val_precision_8: 0.0000e+00\n",
      "Epoch 34/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.1800 - precision_8: 0.0878 - val_loss: 0.2161 - val_precision_8: 0.0000e+00\n",
      "Epoch 35/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.1502 - precision_8: 0.0891 - val_loss: 0.2172 - val_precision_8: 0.0000e+00\n",
      "Epoch 36/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.1548 - precision_8: 0.0873 - val_loss: 0.2186 - val_precision_8: 0.0000e+00\n",
      "Epoch 37/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.1408 - precision_8: 0.0860 - val_loss: 0.2190 - val_precision_8: 0.0000e+00\n",
      "Epoch 38/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.1260 - precision_8: 0.0851 - val_loss: 0.2196 - val_precision_8: 0.0000e+00\n",
      "Epoch 39/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.0982 - precision_8: 0.0860 - val_loss: 0.2215 - val_precision_8: 0.0000e+00\n",
      "Epoch 40/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.0948 - precision_8: 0.0841 - val_loss: 0.2228 - val_precision_8: 0.0000e+00\n",
      "Epoch 41/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.0931 - precision_8: 0.0808 - val_loss: 0.2247 - val_precision_8: 0.0000e+00\n",
      "Epoch 42/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.0767 - precision_8: 0.0801 - val_loss: 0.2257 - val_precision_8: 0.0000e+00\n",
      "Epoch 43/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.0726 - precision_8: 0.0802 - val_loss: 0.2268 - val_precision_8: 0.0000e+00\n",
      "Epoch 44/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.0615 - precision_8: 0.0806 - val_loss: 0.2298 - val_precision_8: 0.0000e+00\n",
      "Epoch 45/250\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.0466 - precision_8: 0.0815 - val_loss: 0.2297 - val_precision_8: 0.0000e+00\n",
      "Epoch 46/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.0448 - precision_8: 0.0799 - val_loss: 0.2327 - val_precision_8: 0.0000e+00\n",
      "Epoch 47/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.0257 - precision_8: 0.0842 - val_loss: 0.2334 - val_precision_8: 0.0000e+00\n",
      "Epoch 48/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.0217 - precision_8: 0.0839 - val_loss: 0.2355 - val_precision_8: 0.0000e+00\n",
      "Epoch 49/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.0180 - precision_8: 0.0814 - val_loss: 0.2365 - val_precision_8: 0.0000e+00\n",
      "Epoch 50/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.0116 - precision_8: 0.0822 - val_loss: 0.2395 - val_precision_8: 0.0000e+00\n",
      "Epoch 51/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.0038 - precision_8: 0.0818 - val_loss: 0.2409 - val_precision_8: 0.0000e+00\n",
      "Epoch 52/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.9946 - precision_8: 0.0818 - val_loss: 0.2426 - val_precision_8: 0.0000e+00\n",
      "Epoch 53/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.9946 - precision_8: 0.0789 - val_loss: 0.2453 - val_precision_8: 0.0000e+00\n",
      "Epoch 54/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.9878 - precision_8: 0.0775 - val_loss: 0.2473 - val_precision_8: 0.0000e+00\n",
      "Epoch 55/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.9859 - precision_8: 0.0794 - val_loss: 0.2503 - val_precision_8: 0.0000e+00\n",
      "Epoch 56/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.9694 - precision_8: 0.0797 - val_loss: 0.2560 - val_precision_8: 0.0000e+00\n",
      "Epoch 57/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.9686 - precision_8: 0.0760 - val_loss: 0.2543 - val_precision_8: 0.0000e+00\n",
      "Epoch 58/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.9584 - precision_8: 0.0796 - val_loss: 0.2557 - val_precision_8: 0.0000e+00\n",
      "Epoch 59/250\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.9496 - precision_8: 0.0813 - val_loss: 0.2617 - val_precision_8: 0.0000e+00\n",
      "Epoch 60/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.9420 - precision_8: 0.0827 - val_loss: 0.2645 - val_precision_8: 0.0000e+00\n",
      "Epoch 61/250\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.9420 - precision_8: 0.0805 - val_loss: 0.2663 - val_precision_8: 0.0000e+00\n",
      "Epoch 62/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.9320 - precision_8: 0.0812 - val_loss: 0.2694 - val_precision_8: 0.0000e+00\n",
      "Epoch 63/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.9244 - precision_8: 0.0842 - val_loss: 0.2699 - val_precision_8: 0.0000e+00\n",
      "Epoch 64/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.9208 - precision_8: 0.0834 - val_loss: 0.2704 - val_precision_8: 0.0000e+00\n",
      "Epoch 65/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.9151 - precision_8: 0.0828 - val_loss: 0.2758 - val_precision_8: 0.0000e+00\n",
      "Epoch 66/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.9084 - precision_8: 0.0841 - val_loss: 0.2773 - val_precision_8: 0.0000e+00\n",
      "Epoch 67/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.8986 - precision_8: 0.0844 - val_loss: 0.2810 - val_precision_8: 0.0000e+00\n",
      "Epoch 68/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.9025 - precision_8: 0.0796 - val_loss: 0.2801 - val_precision_8: 0.0000e+00\n",
      "Epoch 69/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.8974 - precision_8: 0.0812 - val_loss: 0.2861 - val_precision_8: 0.0000e+00\n",
      "Epoch 70/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.8878 - precision_8: 0.0833 - val_loss: 0.2874 - val_precision_8: 0.0000e+00\n",
      "Epoch 71/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.8765 - precision_8: 0.0875 - val_loss: 0.2925 - val_precision_8: 0.0000e+00\n",
      "Epoch 72/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.8766 - precision_8: 0.0867 - val_loss: 0.2949 - val_precision_8: 0.0000e+00\n",
      "Epoch 73/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.8723 - precision_8: 0.0857 - val_loss: 0.2949 - val_precision_8: 0.0000e+00\n",
      "Epoch 74/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.8554 - precision_8: 0.0930 - val_loss: 0.3005 - val_precision_8: 0.0000e+00\n",
      "Epoch 75/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.8574 - precision_8: 0.0918 - val_loss: 0.2993 - val_precision_8: 0.0000e+00\n",
      "Epoch 76/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.8500 - precision_8: 0.0905 - val_loss: 0.3079 - val_precision_8: 0.0000e+00\n",
      "Epoch 77/250\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.8440 - precision_8: 0.0931 - val_loss: 0.3071 - val_precision_8: 0.0000e+00\n",
      "Epoch 78/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.8406 - precision_8: 0.0934 - val_loss: 0.3127 - val_precision_8: 0.0000e+00\n",
      "Epoch 79/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.8341 - precision_8: 0.0926 - val_loss: 0.3146 - val_precision_8: 0.0000e+00\n",
      "Epoch 80/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.8254 - precision_8: 0.0953 - val_loss: 0.3190 - val_precision_8: 0.0000e+00\n",
      "Epoch 81/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.8280 - precision_8: 0.0945 - val_loss: 0.3192 - val_precision_8: 0.0000e+00\n",
      "Epoch 82/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.8247 - precision_8: 0.0941 - val_loss: 0.3221 - val_precision_8: 0.0000e+00\n",
      "Epoch 83/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.8174 - precision_8: 0.0967 - val_loss: 0.3266 - val_precision_8: 0.0000e+00\n",
      "Epoch 84/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.8072 - precision_8: 0.0990 - val_loss: 0.3311 - val_precision_8: 0.0000e+00\n",
      "Epoch 85/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.8014 - precision_8: 0.1001 - val_loss: 0.3282 - val_precision_8: 0.0000e+00\n",
      "Epoch 86/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.8029 - precision_8: 0.1017 - val_loss: 0.3376 - val_precision_8: 0.0000e+00\n",
      "Epoch 87/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7980 - precision_8: 0.1009 - val_loss: 0.3402 - val_precision_8: 0.0000e+00\n",
      "Epoch 88/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7970 - precision_8: 0.0984 - val_loss: 0.3361 - val_precision_8: 0.0000e+00\n",
      "Epoch 89/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7898 - precision_8: 0.1009 - val_loss: 0.3449 - val_precision_8: 0.0000e+00\n",
      "Epoch 90/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7869 - precision_8: 0.1033 - val_loss: 0.3474 - val_precision_8: 0.0455\n",
      "Epoch 91/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7804 - precision_8: 0.1059 - val_loss: 0.3498 - val_precision_8: 0.0000e+00\n",
      "Epoch 92/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7772 - precision_8: 0.1049 - val_loss: 0.3511 - val_precision_8: 0.0000e+00\n",
      "Epoch 93/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7722 - precision_8: 0.1058 - val_loss: 0.3500 - val_precision_8: 0.0000e+00\n",
      "Epoch 94/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7708 - precision_8: 0.1057 - val_loss: 0.3548 - val_precision_8: 0.0000e+00\n",
      "Epoch 95/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7669 - precision_8: 0.1052 - val_loss: 0.3652 - val_precision_8: 0.0755\n",
      "Epoch 96/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7674 - precision_8: 0.1064 - val_loss: 0.3631 - val_precision_8: 0.0000e+00\n",
      "Epoch 97/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7597 - precision_8: 0.1066 - val_loss: 0.3610 - val_precision_8: 0.0000e+00\n",
      "Epoch 98/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7582 - precision_8: 0.1082 - val_loss: 0.3641 - val_precision_8: 0.0000e+00\n",
      "Epoch 99/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7546 - precision_8: 0.1090 - val_loss: 0.3722 - val_precision_8: 0.0631\n",
      "Epoch 100/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7579 - precision_8: 0.1071 - val_loss: 0.3732 - val_precision_8: 0.0723\n",
      "Epoch 101/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7483 - precision_8: 0.1104 - val_loss: 0.3766 - val_precision_8: 0.0642\n",
      "Epoch 102/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7427 - precision_8: 0.1115 - val_loss: 0.3723 - val_precision_8: 0.0000e+00\n",
      "Epoch 103/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7450 - precision_8: 0.1115 - val_loss: 0.3723 - val_precision_8: 0.0000e+00\n",
      "Epoch 104/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7409 - precision_8: 0.1121 - val_loss: 0.3831 - val_precision_8: 0.0657\n",
      "Epoch 105/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7397 - precision_8: 0.1117 - val_loss: 0.3794 - val_precision_8: 0.0000e+00\n",
      "Epoch 106/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7365 - precision_8: 0.1118 - val_loss: 0.3874 - val_precision_8: 0.1234\n",
      "Epoch 107/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7353 - precision_8: 0.1142 - val_loss: 0.3816 - val_precision_8: 0.0000e+00\n",
      "Epoch 108/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7286 - precision_8: 0.1139 - val_loss: 0.3834 - val_precision_8: 0.0000e+00\n",
      "Epoch 109/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7305 - precision_8: 0.1151 - val_loss: 0.3906 - val_precision_8: 0.1469\n",
      "Epoch 110/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7283 - precision_8: 0.1147 - val_loss: 0.3885 - val_precision_8: 0.1529\n",
      "Epoch 111/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7262 - precision_8: 0.1171 - val_loss: 0.3857 - val_precision_8: 0.0000e+00\n",
      "Epoch 112/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.7250 - precision_8: 0.1173 - val_loss: 0.3863 - val_precision_8: 0.1429\n",
      "Epoch 113/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7216 - precision_8: 0.1168 - val_loss: 0.3886 - val_precision_8: 0.1026\n",
      "Epoch 114/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7150 - precision_8: 0.1184 - val_loss: 0.3946 - val_precision_8: 0.1866\n",
      "Epoch 115/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7179 - precision_8: 0.1200 - val_loss: 0.3916 - val_precision_8: 0.1623\n",
      "Epoch 116/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.7155 - precision_8: 0.1197 - val_loss: 0.3880 - val_precision_8: 0.0750\n",
      "Epoch 117/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7137 - precision_8: 0.1199 - val_loss: 0.3908 - val_precision_8: 0.1548\n",
      "Epoch 118/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7073 - precision_8: 0.1224 - val_loss: 0.3928 - val_precision_8: 0.1640\n",
      "Epoch 119/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.7097 - precision_8: 0.1205 - val_loss: 0.3982 - val_precision_8: 0.1971\n",
      "Epoch 120/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.7081 - precision_8: 0.1226 - val_loss: 0.3996 - val_precision_8: 0.1980\n",
      "Epoch 121/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7023 - precision_8: 0.1221 - val_loss: 0.4025 - val_precision_8: 0.1911\n",
      "Epoch 122/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6996 - precision_8: 0.1242 - val_loss: 0.3998 - val_precision_8: 0.1976\n",
      "Epoch 123/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7008 - precision_8: 0.1250 - val_loss: 0.3969 - val_precision_8: 0.1877\n",
      "Epoch 124/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6975 - precision_8: 0.1250 - val_loss: 0.3965 - val_precision_8: 0.1751\n",
      "Epoch 125/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6929 - precision_8: 0.1262 - val_loss: 0.3956 - val_precision_8: 0.1651\n",
      "Epoch 126/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6914 - precision_8: 0.1284 - val_loss: 0.3972 - val_precision_8: 0.1802\n",
      "Epoch 127/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6896 - precision_8: 0.1293 - val_loss: 0.4013 - val_precision_8: 0.1962\n",
      "Epoch 128/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6900 - precision_8: 0.1292 - val_loss: 0.4094 - val_precision_8: 0.1739\n",
      "Epoch 129/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6922 - precision_8: 0.1279 - val_loss: 0.4009 - val_precision_8: 0.1950\n",
      "Epoch 130/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6865 - precision_8: 0.1302 - val_loss: 0.3970 - val_precision_8: 0.1575\n",
      "Epoch 131/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6923 - precision_8: 0.1318 - val_loss: 0.4048 - val_precision_8: 0.1818\n",
      "Epoch 132/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6896 - precision_8: 0.1301 - val_loss: 0.4035 - val_precision_8: 0.1843\n",
      "Epoch 133/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6831 - precision_8: 0.1336 - val_loss: 0.4076 - val_precision_8: 0.1772\n",
      "Epoch 134/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6822 - precision_8: 0.1343 - val_loss: 0.4035 - val_precision_8: 0.1862\n",
      "Epoch 135/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6815 - precision_8: 0.1329 - val_loss: 0.4057 - val_precision_8: 0.1832\n",
      "Epoch 136/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6765 - precision_8: 0.1359 - val_loss: 0.4035 - val_precision_8: 0.1905\n",
      "Epoch 137/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6781 - precision_8: 0.1344 - val_loss: 0.4073 - val_precision_8: 0.1813\n",
      "Epoch 138/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6775 - precision_8: 0.1347 - val_loss: 0.4052 - val_precision_8: 0.1886\n",
      "Epoch 139/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6745 - precision_8: 0.1365 - val_loss: 0.4074 - val_precision_8: 0.1825\n",
      "Epoch 140/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6714 - precision_8: 0.1364 - val_loss: 0.4096 - val_precision_8: 0.1846\n",
      "Epoch 141/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6734 - precision_8: 0.1369 - val_loss: 0.4101 - val_precision_8: 0.1857\n",
      "Epoch 142/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6661 - precision_8: 0.1382 - val_loss: 0.4132 - val_precision_8: 0.1783\n",
      "Epoch 143/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6654 - precision_8: 0.1387 - val_loss: 0.4178 - val_precision_8: 0.1700\n",
      "Epoch 144/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6695 - precision_8: 0.1385 - val_loss: 0.4192 - val_precision_8: 0.1687\n",
      "Epoch 145/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6705 - precision_8: 0.1357 - val_loss: 0.4156 - val_precision_8: 0.1768\n",
      "Epoch 146/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6603 - precision_8: 0.1406 - val_loss: 0.4157 - val_precision_8: 0.1783\n",
      "Epoch 147/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6619 - precision_8: 0.1387 - val_loss: 0.4159 - val_precision_8: 0.1830\n",
      "Epoch 148/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6596 - precision_8: 0.1411 - val_loss: 0.4237 - val_precision_8: 0.1670\n",
      "Epoch 149/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6574 - precision_8: 0.1406 - val_loss: 0.4172 - val_precision_8: 0.1875\n",
      "Epoch 150/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6598 - precision_8: 0.1407 - val_loss: 0.4236 - val_precision_8: 0.1687\n",
      "Epoch 151/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6620 - precision_8: 0.1391 - val_loss: 0.4210 - val_precision_8: 0.1763\n",
      "Epoch 152/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6620 - precision_8: 0.1394 - val_loss: 0.4227 - val_precision_8: 0.1733\n",
      "Epoch 153/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6573 - precision_8: 0.1417 - val_loss: 0.4212 - val_precision_8: 0.1781\n",
      "Epoch 154/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6624 - precision_8: 0.1391 - val_loss: 0.4240 - val_precision_8: 0.1706\n",
      "Epoch 155/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6549 - precision_8: 0.1403 - val_loss: 0.4232 - val_precision_8: 0.1753\n",
      "Epoch 156/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6582 - precision_8: 0.1404 - val_loss: 0.4357 - val_precision_8: 0.1606\n",
      "Epoch 157/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6588 - precision_8: 0.1406 - val_loss: 0.4272 - val_precision_8: 0.1734\n",
      "Epoch 158/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6535 - precision_8: 0.1437 - val_loss: 0.4367 - val_precision_8: 0.1611\n",
      "Epoch 159/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6583 - precision_8: 0.1393 - val_loss: 0.4316 - val_precision_8: 0.1673\n",
      "Epoch 160/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6546 - precision_8: 0.1405 - val_loss: 0.4474 - val_precision_8: 0.0000e+00\n",
      "Epoch 161/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6892 - precision_8: 0.1362 - val_loss: 0.4375 - val_precision_8: 0.1605\n",
      "Epoch 162/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6587 - precision_8: 0.1426 - val_loss: 0.4340 - val_precision_8: 0.1636\n",
      "Epoch 163/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6506 - precision_8: 0.1445 - val_loss: 0.4293 - val_precision_8: 0.1784\n",
      "Epoch 164/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6522 - precision_8: 0.1430 - val_loss: 0.4332 - val_precision_8: 0.1742\n",
      "Epoch 165/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6508 - precision_8: 0.1406 - val_loss: 0.4358 - val_precision_8: 0.1702\n",
      "Epoch 166/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6515 - precision_8: 0.1411 - val_loss: 0.4359 - val_precision_8: 0.1720\n",
      "Epoch 167/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6514 - precision_8: 0.1405 - val_loss: 0.4386 - val_precision_8: 0.1678\n",
      "Epoch 168/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6497 - precision_8: 0.1412 - val_loss: 0.4422 - val_precision_8: 0.1638\n",
      "Epoch 169/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6473 - precision_8: 0.1422 - val_loss: 0.4392 - val_precision_8: 0.1752\n",
      "Epoch 170/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6529 - precision_8: 0.1414 - val_loss: 0.4447 - val_precision_8: 0.1629\n",
      "Epoch 171/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6459 - precision_8: 0.1428 - val_loss: 0.4412 - val_precision_8: 0.1733\n",
      "Epoch 172/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6486 - precision_8: 0.1419 - val_loss: 0.4420 - val_precision_8: 0.1680\n",
      "Epoch 173/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6471 - precision_8: 0.1428 - val_loss: 0.4414 - val_precision_8: 0.1720\n",
      "Epoch 174/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6439 - precision_8: 0.1431 - val_loss: 0.4443 - val_precision_8: 0.1642\n",
      "Epoch 175/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6414 - precision_8: 0.1435 - val_loss: 0.4433 - val_precision_8: 0.1694\n",
      "Epoch 176/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6451 - precision_8: 0.1427 - val_loss: 0.4464 - val_precision_8: 0.1634\n",
      "Epoch 177/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6415 - precision_8: 0.1429 - val_loss: 0.4477 - val_precision_8: 0.1643\n",
      "Epoch 178/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6398 - precision_8: 0.1421 - val_loss: 0.4518 - val_precision_8: 0.1605\n",
      "Epoch 179/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6408 - precision_8: 0.1424 - val_loss: 0.4524 - val_precision_8: 0.1607\n",
      "Epoch 180/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6399 - precision_8: 0.1419 - val_loss: 0.4520 - val_precision_8: 0.1610\n",
      "Epoch 181/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6399 - precision_8: 0.1421 - val_loss: 0.4511 - val_precision_8: 0.1650\n",
      "Epoch 182/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6351 - precision_8: 0.1443 - val_loss: 0.4527 - val_precision_8: 0.1693\n",
      "Epoch 183/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6360 - precision_8: 0.1428 - val_loss: 0.4548 - val_precision_8: 0.1631\n",
      "Epoch 184/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6354 - precision_8: 0.1424 - val_loss: 0.4561 - val_precision_8: 0.1670\n",
      "Epoch 185/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6344 - precision_8: 0.1433 - val_loss: 0.4726 - val_precision_8: 0.1518\n",
      "Epoch 186/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6410 - precision_8: 0.1392 - val_loss: 0.4591 - val_precision_8: 0.1652\n",
      "Epoch 187/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6339 - precision_8: 0.1424 - val_loss: 0.4629 - val_precision_8: 0.1573\n",
      "Epoch 188/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6328 - precision_8: 0.1424 - val_loss: 0.4617 - val_precision_8: 0.1657\n",
      "Epoch 189/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6327 - precision_8: 0.1416 - val_loss: 0.4621 - val_precision_8: 0.1615\n",
      "Epoch 190/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6328 - precision_8: 0.1411 - val_loss: 0.4689 - val_precision_8: 0.1535\n",
      "Epoch 191/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6374 - precision_8: 0.1395 - val_loss: 0.4720 - val_precision_8: 0.1526\n",
      "Epoch 192/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6386 - precision_8: 0.1396 - val_loss: 0.4651 - val_precision_8: 0.1624\n",
      "Epoch 193/250\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.6340 - precision_8: 0.1393 - val_loss: 0.4671 - val_precision_8: 0.1681\n",
      "Epoch 194/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6311 - precision_8: 0.1417 - val_loss: 0.4696 - val_precision_8: 0.1544\n",
      "Epoch 195/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6297 - precision_8: 0.1426 - val_loss: 0.4678 - val_precision_8: 0.1588\n",
      "Epoch 196/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6363 - precision_8: 0.1401 - val_loss: 0.4684 - val_precision_8: 0.1604\n",
      "Epoch 197/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6267 - precision_8: 0.1445 - val_loss: 0.4705 - val_precision_8: 0.1668\n",
      "Epoch 198/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6303 - precision_8: 0.1417 - val_loss: 0.4721 - val_precision_8: 0.1545\n",
      "Epoch 199/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6334 - precision_8: 0.1420 - val_loss: 0.4843 - val_precision_8: 0.1496\n",
      "Epoch 200/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6348 - precision_8: 0.1402 - val_loss: 0.4697 - val_precision_8: 0.1573\n",
      "Epoch 201/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6307 - precision_8: 0.1397 - val_loss: 0.4732 - val_precision_8: 0.1544\n",
      "Epoch 202/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6295 - precision_8: 0.1431 - val_loss: 0.4731 - val_precision_8: 0.1554\n",
      "Epoch 203/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6279 - precision_8: 0.1417 - val_loss: 0.4755 - val_precision_8: 0.1646\n",
      "Epoch 204/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6274 - precision_8: 0.1420 - val_loss: 0.4752 - val_precision_8: 0.1613\n",
      "Epoch 205/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6278 - precision_8: 0.1418 - val_loss: 0.4753 - val_precision_8: 0.1581\n",
      "Epoch 206/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6226 - precision_8: 0.1424 - val_loss: 0.4780 - val_precision_8: 0.1637\n",
      "Epoch 207/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6268 - precision_8: 0.1407 - val_loss: 0.4785 - val_precision_8: 0.1527\n",
      "Epoch 208/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6253 - precision_8: 0.1446 - val_loss: 0.4768 - val_precision_8: 0.1573\n",
      "Epoch 209/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6284 - precision_8: 0.1391 - val_loss: 0.4840 - val_precision_8: 0.1678\n",
      "Epoch 210/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6227 - precision_8: 0.1432 - val_loss: 0.4828 - val_precision_8: 0.1514\n",
      "Epoch 211/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6232 - precision_8: 0.1435 - val_loss: 0.4813 - val_precision_8: 0.1611\n",
      "Epoch 212/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6225 - precision_8: 0.1422 - val_loss: 0.4807 - val_precision_8: 0.1565\n",
      "Epoch 213/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6223 - precision_8: 0.1433 - val_loss: 0.4915 - val_precision_8: 0.1712\n",
      "Epoch 214/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6271 - precision_8: 0.1418 - val_loss: 0.4940 - val_precision_8: 0.1484\n",
      "Epoch 215/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6261 - precision_8: 0.1392 - val_loss: 0.4799 - val_precision_8: 0.1565\n",
      "Epoch 216/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6197 - precision_8: 0.1428 - val_loss: 0.4802 - val_precision_8: 0.1552\n",
      "Epoch 217/250\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.6169 - precision_8: 0.1450 - val_loss: 0.5152 - val_precision_8: 0.1462\n",
      "Epoch 218/250\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.6314 - precision_8: 0.1422 - val_loss: 0.5050 - val_precision_8: 0.1727\n",
      "Epoch 219/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6249 - precision_8: 0.1428 - val_loss: 0.4904 - val_precision_8: 0.1501\n",
      "Epoch 220/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6220 - precision_8: 0.1394 - val_loss: 0.4906 - val_precision_8: 0.1606\n",
      "Epoch 221/250\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.6230 - precision_8: 0.1426 - val_loss: 0.4906 - val_precision_8: 0.1502\n",
      "Epoch 222/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6197 - precision_8: 0.1421 - val_loss: 0.5065 - val_precision_8: 0.1722\n",
      "Epoch 223/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6241 - precision_8: 0.1416 - val_loss: 0.5162 - val_precision_8: 0.1451\n",
      "Epoch 224/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6322 - precision_8: 0.1390 - val_loss: 0.5032 - val_precision_8: 0.1472\n",
      "Epoch 225/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6196 - precision_8: 0.1423 - val_loss: 0.4950 - val_precision_8: 0.1499\n",
      "Epoch 226/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6154 - precision_8: 0.1408 - val_loss: 0.4927 - val_precision_8: 0.1523\n",
      "Epoch 227/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6207 - precision_8: 0.1401 - val_loss: 0.4935 - val_precision_8: 0.1502\n",
      "Epoch 228/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6168 - precision_8: 0.1417 - val_loss: 0.4972 - val_precision_8: 0.1580\n",
      "Epoch 229/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6210 - precision_8: 0.1380 - val_loss: 0.4963 - val_precision_8: 0.1557\n",
      "Epoch 230/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6179 - precision_8: 0.1419 - val_loss: 0.4950 - val_precision_8: 0.1510\n",
      "Epoch 231/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6187 - precision_8: 0.1409 - val_loss: 0.4999 - val_precision_8: 0.1482\n",
      "Epoch 232/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6171 - precision_8: 0.1404 - val_loss: 0.5101 - val_precision_8: 0.1643\n",
      "Epoch 233/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6184 - precision_8: 0.1409 - val_loss: 0.6178 - val_precision_8: 0.0011\n",
      "Epoch 234/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6727 - precision_8: 0.1291 - val_loss: 0.4950 - val_precision_8: 0.1512\n",
      "Epoch 235/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6531 - precision_8: 0.1359 - val_loss: 0.5398 - val_precision_8: 0.1550\n",
      "Epoch 236/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6273 - precision_8: 0.1426 - val_loss: 0.4964 - val_precision_8: 0.1528\n",
      "Epoch 237/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6167 - precision_8: 0.1438 - val_loss: 0.4965 - val_precision_8: 0.1510\n",
      "Epoch 238/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6146 - precision_8: 0.1440 - val_loss: 0.5012 - val_precision_8: 0.1497\n",
      "Epoch 239/250\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.6122 - precision_8: 0.1430 - val_loss: 0.5039 - val_precision_8: 0.1519\n",
      "Epoch 240/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6155 - precision_8: 0.1399 - val_loss: 0.5097 - val_precision_8: 0.1555\n",
      "Epoch 241/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6134 - precision_8: 0.1402 - val_loss: 0.5170 - val_precision_8: 0.1593\n",
      "Epoch 242/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6157 - precision_8: 0.1410 - val_loss: 0.5372 - val_precision_8: 0.1726\n",
      "Epoch 243/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6176 - precision_8: 0.1403 - val_loss: 0.5263 - val_precision_8: 0.1439\n",
      "Epoch 244/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6306 - precision_8: 0.1357 - val_loss: 0.5218 - val_precision_8: 0.1444\n",
      "Epoch 245/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6181 - precision_8: 0.1405 - val_loss: 0.5130 - val_precision_8: 0.1539\n",
      "Epoch 246/250\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.6128 - precision_8: 0.1399 - val_loss: 0.5142 - val_precision_8: 0.1538\n",
      "Epoch 247/250\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.6123 - precision_8: 0.1406 - val_loss: 0.5117 - val_precision_8: 0.1474\n",
      "Epoch 248/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6144 - precision_8: 0.1397 - val_loss: 0.5415 - val_precision_8: 0.1714\n",
      "Epoch 249/250\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6229 - precision_8: 0.1386 - val_loss: 0.5550 - val_precision_8: 0.1776\n",
      "Epoch 250/250\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6206 - precision_8: 0.1408 - val_loss: 0.5128 - val_precision_8: 0.1470\n"
     ]
    }
   ],
   "source": [
    "weighted_model = make_model()\n",
    "weighted_model.load_weights(initial_weights)\n",
    "EPOCHS = 250\n",
    "weighted_history = weighted_model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(val_features, val_labels),\n",
    "    # The class weights go here\n",
    "    class_weight=class_weight,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 15:25:32.190814: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 11ms/step\n",
      "9/9 [==============================] - 0s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "train_predictions_weighted = weighted_model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_weighted = weighted_model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 8ms/step - loss: 0.5105 - precision_8: 0.1462\n",
      "loss :  0.510508120059967\n",
      "precision_8 :  0.14622555673122406\n",
      "\n",
      "Legitimate Transactions Detected (True Negatives):  126814\n",
      "Legitimate Transactions Incorrectly Detected (False Positives):  6159\n",
      "Fraudulent Transactions Missed (False Negatives):  6434\n",
      "Fraudulent Transactions Detected (True Positives):  1218\n",
      "Total Fraudulent Transactions:  7652\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weighted_results = weighted_model.evaluate(test_features, test_labels,\n",
    "                                           batch_size=BATCH_SIZE, verbose=1)\n",
    "for name, value in zip(weighted_model.metrics_names, weighted_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(test_labels, test_predictions_weighted,threshold=0.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_mac",
   "language": "python",
   "name": "tf_mac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
